{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Leaf Disease Classification en Kaggle\n",
    "\n",
    "## Competencia\n",
    "- **Nombre:** Cassava Leaf Disease Classification  \n",
    "- **Enlace:** https://www.kaggle.com/competitions/cassava-leaf-disease-classification/overview  \n",
    "- **Objetivo:** Clasificar imágenes de hojas de yuca en cinco categorías (enfermedad o sana), contribuyendo al control de plagas y al incremento de la productividad agrícola.\n",
    "\n",
    "## Notebook de entrenamiento\n",
    "- **Autor:** Ricardo Lopera  \n",
    "- **Enlace:** https://www.kaggle.com/code/ricardolopera/leaf-disease-classification  \n",
    "\n",
    "> **Nota importante:**  \n",
    "> Al final del desarrollo en Kaggle Notebook se produjo un error al guardar el borrador. No obstante, el repositorio que acompaña este notebook contiene la versión completa del entrenamiento.\n",
    "\n",
    "## Modelo obtenido\n",
    "\n",
    "- **Enlace:** https://drive.google.com/file/d/1h9Jz0Qex0zRKHVqLO5vvnSm1GAOsEfeB/view?usp=sharing \n",
    "\n",
    "## Recursos de cómputo: TPU en Kaggle\n",
    "- Kaggle ofrece **un núcleo de TPU v3** (Tensor Processing Unit) por notebook de forma gratuita.  \n",
    "- Una TPU es un acelerador especializado diseñado por Google para optimizar operaciones de TensorFlow y acelerar el entrenamiento de redes neuronales profundas.  \n",
    "- Aunque la TPU de Kaggle consta de un pod de 8 núcleos, en este proyecto se utilizó únicamente **un solo núcleo**, por lo que el tiempo de entrenamiento fue de **casi 3 horas**.\n",
    "\n",
    "---\n",
    "\n",
    "### Cómo funcionan las TPU en Kaggle\n",
    "1. **Selección del acelerador:**  \n",
    "   En la configuración del notebook, se elige “TPU” en lugar de CPU o GPU.  \n",
    "2. **Distribución automática:**  \n",
    "   TensorFlow detecta el entorno TPU y ejecuta las operaciones de forma distribuida sobre los núcleos disponibles.  \n",
    "3. **Ventajas principales:**  \n",
    "   - Mejora significativa en velocidad de entrenamiento y evaluación.  \n",
    "   - Mayor eficiencia energética frente a GPU/CPU convencionales.  \n",
    "4. **Consideraciones:**  \n",
    "   - Hay que usar estrategias de distribución (`tf.distribute.TPUStrategy`).  \n",
    "   - El límite de memoria y de tiempo de sesión de Kaggle Notebook aplica también a TPU.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clasificación de Enfermedades en Hojas de Yuca usando Vision Transformers\n",
    "\n",
    "## Descripción del Proyecto\n",
    "Este notebook implementa un modelo de clasificación de enfermedades en hojas de yuca utilizando Vision Transformers (ViT), participando en la competencia \"Cassava Leaf Disease Classification\" de Kaggle.\n",
    "\n",
    "## Componentes Principales\n",
    "\n",
    "### 1. Preparación de Datos\n",
    "- Carga del dataset desde Kaggle\n",
    "- Implementación de data augmentation con transformaciones como:\n",
    "  - Volteos horizontales y verticales\n",
    "  - Rotaciones aleatorias\n",
    "  - Transformaciones afines\n",
    "  - Normalización de imágenes\n",
    "\n",
    "### 2. Arquitectura del Modelo\n",
    "- Implementación de ViTBase16 (Vision Transformer)\n",
    "- Adaptación de la capa final para 5 clases (enfermedades)\n",
    "\n",
    "### 3. Entrenamiento\n",
    "- Utilización de TPU para aceleración del entrenamiento\n",
    "- Implementación de entrenamiento distribuido\n",
    "- Hiperparámetros clave:\n",
    "  - Tamaño de imagen: 224x224\n",
    "  - Batch size: 16\n",
    "  - Learning rate: 2e-05\n",
    "  - Épocas: 10\n",
    "\n",
    "### 4. Optimizaciones\n",
    "- Manejo eficiente de memoria con garbage collection\n",
    "- Implementación de early stopping\n",
    "- Guardado del mejor modelo basado en pérdida de validación\n",
    "\n",
    "### 5. Monitoreo\n",
    "- Seguimiento de métricas:\n",
    "  - Pérdida de entrenamiento y validación\n",
    "  - Precisión de entrenamiento y validación\n",
    "- Visualización de la distribución de clases\n",
    "\n",
    "## Resultados\n",
    "El modelo se entrenó durante aproximadamente 3 horas en TPU, implementando técnicas de data augmentation y optimización para mejorar la robustez y precisión en la clasificación de enfermedades en hojas de yuca."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-07-22T10:57:35.636615Z",
     "iopub.status.busy": "2025-07-22T10:57:35.636277Z",
     "iopub.status.idle": "2025-07-22T10:58:13.883595Z",
     "shell.execute_reply": "2025-07-22T10:58:13.879026Z",
     "shell.execute_reply.started": "2025-07-22T10:57:35.636587Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#Importamos las librerias necesarias\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import timm\n",
    "import gc\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "from datetime import datetime\n",
    "from PIL import Image\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn import model_selection, metrics\n",
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-22T10:58:13.884913Z",
     "iopub.status.busy": "2025-07-22T10:58:13.884604Z",
     "iopub.status.idle": "2025-07-22T10:58:13.916961Z",
     "shell.execute_reply": "2025-07-22T10:58:13.912513Z",
     "shell.execute_reply.started": "2025-07-22T10:58:13.884890Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Tomamos los datos de Kaggle y los cargamos en un DataFrame\n",
    "\n",
    "Data = \"/kaggle/input/cassava-leaf-disease-classification\"\n",
    "Train = \"/kaggle/input/cassava-leaf-disease-classification/train_images/\"\n",
    "Test = \"/kaggle/input/cassava-leaf-disease-classification/test_images/\"\n",
    "\n",
    "\n",
    "df = pd.read_csv(os.path.join(Data, \"train.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para este proyecto se hará uso del TPU proporcionado por Kaggle, para lo cual implementaremos primero paralelización del TPU. Esto nos permitirá acelerar el entrenamiento de modelos de deep learning utilizando la librería Torch XLA, que facilita la integración de PyTorch con dispositivos TPU. La paralelización se logra distribuyendo los datos y el proceso de entrenamiento entre los diferentes núcleos del TPU, optimizando así el uso de recursos y reduciendo significativamente el tiempo de entrenamiento. Además, se emplearán técnicas como el uso de DataLoader y ParallelLoader para manejar eficientemente los datos durante el entrenamiento distribuido."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`XLA_USE_BF16`: Activa el uso de precisión mixta (BF16), lo que acelera el entrenamiento y reduce el consumo de memoria en TPU.\n",
    "`XLA_TENSOR_ALLOCATOR_MAXSIZE`: Limita el tamaño máximo del asignador de tensores en XLA, ayudando a controlar el uso de memoria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-22T10:58:13.918966Z",
     "iopub.status.busy": "2025-07-22T10:58:13.918748Z",
     "iopub.status.idle": "2025-07-22T10:58:13.934708Z",
     "shell.execute_reply": "2025-07-22T10:58:13.928054Z",
     "shell.execute_reply.started": "2025-07-22T10:58:13.918946Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "os.environ[\"XLA_USE_BF16\"] = \"1\"\n",
    "os.environ[\"XLA_TENSOR_ALLOCATOR_MAXSIZE\"] = \"100000000\"\n",
    "\n",
    "# Para la reproducibilidad de los resultados hacemos uso de una semilla fija\n",
    "# Esto asegura que los resultados sean consistentes en diferentes ejecuciones\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definición de Parámetros: `IMG_SIZE`, `BATCH_SIZE`, `LR`, y `N_EPOCHS`\n",
    "\n",
    "En este apartado, vamos a definir los parámetros clave para configurar el entrenamiento de nuestro modelo Vision Transformer (ViT) para la clasificación de enfermedades en hojas. Estos parámetros influirán directamente en el comportamiento y rendimiento del modelo durante el entrenamiento.\n",
    "\n",
    "1. **`IMG_SIZE = 224`**:\n",
    "   El parámetro `IMG_SIZE` define el tamaño de las imágenes de entrada que se alimentan al modelo. Dado que estamos utilizando un **Vision Transformer (ViT)**, que requiere imágenes de un tamaño específico, hemos decidido establecer un tamaño de imagen de **224x224 píxeles**. Este es un tamaño comúnmente utilizado en muchos modelos preentrenados, lo que ayuda a optimizar el tiempo de entrenamiento sin perder demasiada resolución en las imágenes.\n",
    "\n",
    "   - **Razón para elegir `224x224`**: Este tamaño es un compromiso entre calidad y eficiencia computacional. Permite capturar suficiente detalle en las imágenes para tareas de clasificación, mientras mantiene un tamaño manejable para los recursos computacionales.\n",
    "\n",
    "2. **`BATCH_SIZE = 16`**:\n",
    "   El `BATCH_SIZE` determina cuántas imágenes serán procesadas en cada iteración del entrenamiento. Con un `batch_size` de **16**, el modelo procesará 16 imágenes simultáneamente en cada paso, actualizando los parámetros basados en el cálculo de los gradientes promedio. Un tamaño de lote más pequeño (como 16) ayuda a reducir el consumo de memoria y puede mejorar la generalización, pero también puede hacer que el entrenamiento sea más ruidoso debido a la mayor variabilidad en los gradientes.\n",
    "\n",
    "   - **Consideraciones para elegir `16`**: Un tamaño de 16 es una opción común que ofrece un buen equilibrio entre eficiencia computacional y generalización en modelos de visión por computadora. En algunos casos, podría ser necesario ajustar este valor dependiendo del tamaño de las imágenes o la memoria disponible.\n",
    "\n",
    "3. **`LR = 2e-05`** (Learning Rate):\n",
    "   La **tasa de aprendizaje** (`LR`) controla qué tan grande es el paso que da el modelo en cada actualización de los parámetros durante el entrenamiento. Un valor de **2e-05** indica que el modelo hará pequeñas actualizaciones en cada iteración. Esto puede ser útil para evitar que el modelo dé pasos demasiado grandes, lo que podría llevar a una mala convergencia o incluso hacer que el entrenamiento se vuelva inestable.\n",
    "\n",
    "   - **Razón para elegir `2e-05`**: Este valor es un buen punto de partida basado en modelos preentrenados de ViT. Sin embargo, la tasa de aprendizaje podría ajustarse según el comportamiento observado en el entrenamiento. Si el modelo se entrena demasiado lento, se podría intentar aumentar la tasa de aprendizaje; si el modelo se vuelve inestable, se debería reducir.\n",
    "\n",
    "4. **`N_EPOCHS = 10`**:\n",
    "   El número de **`epochs`** define cuántas veces todo el conjunto de datos se pasará a través del modelo. Un valor de **10** implica que el modelo entrenará durante 10 pasadas completas sobre el conjunto de entrenamiento. El número de epochs es crucial porque un valor bajo podría resultar en un entrenamiento insuficiente, mientras que uno alto podría dar lugar a sobreajuste (overfitting).\n",
    "\n",
    "   - **Consideraciones para elegir `10`**: En este caso, 10 épocas es un valor inicial razonable para observar cómo el modelo se ajusta a los datos. Dependiendo de los resultados, podríamos aumentar o disminuir este valor. Es importante observar el rendimiento en el conjunto de validación para determinar si el modelo está aprendiendo adecuadamente o si es necesario más entrenamiento.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-22T10:58:13.935599Z",
     "iopub.status.busy": "2025-07-22T10:58:13.935356Z",
     "iopub.status.idle": "2025-07-22T10:58:13.943724Z",
     "shell.execute_reply": "2025-07-22T10:58:13.940326Z",
     "shell.execute_reply.started": "2025-07-22T10:58:13.935576Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "IMG_SIZE = 224\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "LR = 2e-05\n",
    "\n",
    "N_EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-22T10:58:13.945793Z",
     "iopub.status.busy": "2025-07-22T10:58:13.945510Z",
     "iopub.status.idle": "2025-07-22T10:58:13.995025Z",
     "shell.execute_reply": "2025-07-22T10:58:13.991144Z",
     "shell.execute_reply.started": "2025-07-22T10:58:13.945770Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000015157.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000201771.jpg</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100042118.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000723321.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000812911.jpg</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         image_id  label\n",
       "0  1000015157.jpg      0\n",
       "1  1000201771.jpg      3\n",
       "2   100042118.jpg      1\n",
       "3  1000723321.jpg      1\n",
       "4  1000812911.jpg      3"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Veamos como son los datos \n",
    "df = pd.read_csv(os.path.join(Data, \"train.csv\"))\n",
    "\n",
    "# Realizamos la separacion de train y test:\n",
    "train_df, valid_df = model_selection.train_test_split(df, \n",
    "                                                      test_size=0.1, \n",
    "                                                      random_state=42, \n",
    "                                                      stratify=df.label.values)\n",
    "\n",
    "df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-22T10:58:13.997391Z",
     "iopub.status.busy": "2025-07-22T10:58:13.997175Z",
     "iopub.status.idle": "2025-07-22T10:58:14.287851Z",
     "shell.execute_reply": "2025-07-22T10:58:14.283514Z",
     "shell.execute_reply.started": "2025-07-22T10:58:13.997371Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Graficamos el diagrama de barras\n",
    "df.label.value_counts().plot(kind=\"bar\")\n",
    "\n",
    "label_counts = df.label.value_counts()\n",
    "\n",
    "# Graficamos el pie chart\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.pie(label_counts.values, labels=label_counts.index, autopct='%1.1f%%', colors=sns.color_palette(\"Set2\", len(label_counts)))\n",
    "\n",
    "plt.title('Distribución de Etiquetas en el Conjunto de Datos', fontsize=16)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conjunto de Datos Cassava\n",
    "\n",
    "El código a continuación define una clase llamada `CassavaDataset`, que facilita la creación de un conjunto de datos personalizado en PyTorch. Esta clase está diseñada específicamente para cargar y preprocesar imágenes del conjunto de datos **Cassava**, que se utilizan para entrenar o evaluar modelos de clasificación de enfermedades en plantas de cassava.\n",
    "\n",
    "1. **Inicialización (`__init__`)**:\n",
    "   - El constructor de la clase recibe varios parámetros: \n",
    "     - `df`: un DataFrame que contiene los nombres de las imágenes y sus etiquetas.\n",
    "     - `data_path`: la ruta donde se almacenan las imágenes. Por defecto, se utiliza `Data`.\n",
    "     - `mode`: un valor que indica si el conjunto de datos es de entrenamiento (\"train\") o prueba (\"test\").\n",
    "     - `transforms`: un parámetro opcional que permite aplicar transformaciones de preprocesamiento (como normalización o aumentos de datos) a las imágenes antes de ser procesadas por el modelo.\n",
    "   - Dependiendo del valor de `mode`, se selecciona el directorio adecuado (`train_images` o `test_images`) donde se encuentran las imágenes.\n",
    "\n",
    "2. **Tamaño del Conjunto de Datos (`__len__`)**:\n",
    "   - El método `__len__` devuelve el número total de imágenes en el conjunto de datos, que se obtiene del DataFrame `df`. Esto es útil para saber cuántas muestras hay disponibles para el entrenamiento o la evaluación.\n",
    "\n",
    "3. **Obtención de un Ítem del Conjunto de Datos (`__getitem__`)**:\n",
    "   - El método `__getitem__` toma un índice y retorna una imagen y su etiqueta correspondiente.\n",
    "   - Primero, se construye la ruta completa de la imagen basándose en el índice y el directorio de imágenes.\n",
    "   - Luego, se carga la imagen desde el disco y se convierte a formato RGB.\n",
    "   - Si se han definido transformaciones (como redimensionamiento o normalización), estas se aplican a la imagen antes de devolverla.\n",
    "   - Finalmente, el método retorna tanto la imagen procesada como su etiqueta.\n",
    "\n",
    "En resumen, esta clase permite cargar y transformar eficientemente las imágenes del conjunto de datos Cassava, facilitando su uso en el entrenamiento y evaluación de modelos de aprendizaje automático en PyTorch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-22T10:58:14.290010Z",
     "iopub.status.busy": "2025-07-22T10:58:14.289558Z",
     "iopub.status.idle": "2025-07-22T10:58:14.300418Z",
     "shell.execute_reply": "2025-07-22T10:58:14.296515Z",
     "shell.execute_reply.started": "2025-07-22T10:58:14.289987Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class CassavaDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, df, data_path= Data, mode=\"train\", transforms=None):\n",
    "        super().__init__()\n",
    "        self.df_data = df.values\n",
    "        self.data_path = data_path\n",
    "        self.transforms = transforms\n",
    "        self.mode = mode\n",
    "        self.data_dir = \"train_images\" if mode == \"train\" else \"test_images\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df_data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_name, label = self.df_data[index]\n",
    "        img_path = os.path.join(self.data_path, self.data_dir, img_name)\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            image = self.transforms(img)\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Augmentation\n",
    "\n",
    "El código a continuación define dos conjuntos de transformaciones para los datos de entrenamiento y validación. Estas transformaciones son aplicadas a las imágenes antes de ser alimentadas al modelo, con el fin de mejorar su generalización y ayudar a prevenir el sobreajuste (overfitting) al introducir variaciones en los datos.\n",
    "\n",
    "1. **Transformaciones para el Conjunto de Entrenamiento (`transforms_train`)**:\n",
    "   - **`Resize`**: Redimensiona las imágenes a un tamaño específico, definido por `IMG_SIZE` (que es un valor previamente definido, como 224x224 píxeles). Esto asegura que todas las imágenes tengan un tamaño uniforme antes de ser procesadas.\n",
    "   - **`RandomHorizontalFlip`**: Realiza un volteo horizontal de las imágenes con una probabilidad del 30%. Esto ayuda a que el modelo sea menos sensible a la orientación de las imágenes.\n",
    "   - **`RandomVerticalFlip`**: Realiza un volteo vertical de las imágenes con una probabilidad del 30%. De nuevo, esto aumenta la variabilidad de las imágenes y mejora la robustez del modelo.\n",
    "   - **`RandomRotation`**: Rota las imágenes aleatoriamente en un rango de hasta 10 grados. Esta transformación es útil para simular variaciones en la orientación de las hojas.\n",
    "   - **`RandomAffine`**: Aplica una transformación afín aleatoria en las imágenes, con un rango de 10 grados. Esto puede incluir cambios en la escala y rotación de la imagen.\n",
    "   - **`RandomResizedCrop`**: Recorta aleatoriamente una porción de la imagen y la redimensiona a `IMG_SIZE`. Esto ayuda a simular variaciones en el encuadre y la escala de los objetos.\n",
    "   - **`ToTensor`**: Convierte la imagen en un tensor de PyTorch, lo que la prepara para ser utilizada en un modelo de aprendizaje automático.\n",
    "   - **`Normalize`**: Normaliza las imágenes utilizando los valores de media y desviación estándar de los canales de color (Rojo, Verde y Azul). Esto es importante para que el modelo pueda aprender de manera más eficiente.\n",
    "\n",
    "2. **Transformaciones para el Conjunto de Validación (`transforms_valid`)**:\n",
    "   - **`Resize`**: Similar al conjunto de entrenamiento, redimensiona las imágenes a `IMG_SIZE`.\n",
    "   - **`ToTensor`**: Convierte las imágenes en tensores de PyTorch.\n",
    "   - **`Normalize`**: Normaliza las imágenes utilizando los mismos valores de media y desviación estándar que en el conjunto de entrenamiento, para asegurar que las imágenes de validación tengan la misma escala que las de entrenamiento.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-22T10:58:14.302780Z",
     "iopub.status.busy": "2025-07-22T10:58:14.302451Z",
     "iopub.status.idle": "2025-07-22T10:58:14.315971Z",
     "shell.execute_reply": "2025-07-22T10:58:14.311247Z",
     "shell.execute_reply.started": "2025-07-22T10:58:14.302737Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Tiempo para realizar aumentos de datos (Data Augmentation)\n",
    "\n",
    "# Conjunto de transformaciones para el conjunto de entrenamiento\n",
    "transforms_train = transforms.Compose(\n",
    "[\n",
    "    # Redimensiona la imagen al tamaño previamente especificado en IMG_SIZE\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    \n",
    "    # Voltea aleatoriamente las imágenes de manera horizontal con una probabilidad del 30%\n",
    "    transforms.RandomHorizontalFlip(p=0.3),\n",
    "    \n",
    "    # Voltea aleatoriamente las imágenes de manera vertical con una probabilidad del 30%\n",
    "    transforms.RandomVerticalFlip(p=0.3),\n",
    "    \n",
    "    # Rota aleatoriamente las imágenes hasta 10 grados\n",
    "    transforms.RandomRotation(10),\n",
    "    \n",
    "    # Aplica una transformación afín aleatoria de hasta 10 grados\n",
    "    transforms.RandomAffine(10),\n",
    "    \n",
    "    # Recorta aleatoriamente las imágenes y las redimensiona al tamaño especificado por IMG_SIZE\n",
    "    transforms.RandomResizedCrop(IMG_SIZE),\n",
    "    \n",
    "    # Convierte la imagen en un tensor, adecuado para el modelo en PyTorch\n",
    "    transforms.ToTensor(),\n",
    "    \n",
    "    # Normaliza la imagen usando la media y desviación estándar de los canales RGB\n",
    "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "]\n",
    ")\n",
    "\n",
    "# Conjunto de transformaciones para el conjunto de validación\n",
    "transforms_valid = transforms.Compose(\n",
    "[\n",
    "    # Redimensiona la imagen al tamaño especificado en IMG_SIZE\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    \n",
    "    # Convierte la imagen en un tensor\n",
    "    transforms.ToTensor(),\n",
    "    \n",
    "    # Normaliza la imagen utilizando la media y desviación estándar de los canales RGB\n",
    "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo ViTBase16\n",
    "\n",
    "El código a continuación define una clase llamada `ViTBase16`, que implementa un modelo **Vision Transformer (ViT)** para la clasificación de imágenes utilizando PyTorch. Este modelo está basado en la arquitectura **ViT Base 16**, con imágenes de entrada redimensionadas a 224x224 píxeles y una división en 16 parches.\n",
    "\n",
    "1. **Inicialización (`__init__`)**:\n",
    "   - El constructor de la clase define el modelo Vision Transformer utilizando la función `timm.create_model` de la biblioteca `timm`, que carga el modelo ViT con parches de tamaño 16x16.\n",
    "   - Si se indica `pretrained=True`, se cargan los pesos preentrenados desde una ruta específica, lo que permite transferir el conocimiento de modelos previamente entrenados en grandes conjuntos de datos.\n",
    "   - Se ajusta la capa de salida (`model.head`) para que tenga el número de clases especificado por `n_classes`.\n",
    "\n",
    "2. **Paso hacia adelante (`forward`)**:\n",
    "   - El método `forward` define cómo los datos de entrada son pasados a través del modelo para obtener las predicciones. Este método es invocado automáticamente durante el entrenamiento y la validación.\n",
    "\n",
    "3. **Entrenamiento por Época (`train_one_epoch`)**:\n",
    "   - Este método ejecuta una iteración completa de entrenamiento:\n",
    "     - Inicializa las variables de pérdida y precisión para la época.\n",
    "     - Realiza la pasada hacia adelante para obtener las predicciones.\n",
    "     - Calcula la pérdida y la precisión.\n",
    "     - Realiza la pasada hacia atrás para actualizar los gradientes.\n",
    "     - Ajusta los parámetros del modelo utilizando el optimizador.\n",
    "   - Si el dispositivo es de tipo `xla`, se emplea un método optimizado para trabajar en entornos como Google Cloud.\n",
    "\n",
    "4. **Validación por Época (`validate_one_epoch`)**:\n",
    "   - Este método realiza la validación del modelo en una época, donde se calcula la pérdida y precisión sin actualizar los gradientes, permitiendo evaluar el rendimiento del modelo en datos no vistos durante el entrenamiento.\n",
    "   - La validación se ejecuta en modo de evaluación para evitar cambios en los pesos del modelo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-22T10:58:14.318310Z",
     "iopub.status.busy": "2025-07-22T10:58:14.318095Z",
     "iopub.status.idle": "2025-07-22T10:58:14.335801Z",
     "shell.execute_reply": "2025-07-22T10:58:14.330323Z",
     "shell.execute_reply.started": "2025-07-22T10:58:14.318289Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class ViTBase16(nn.Module):\n",
    "    def __init__(self, n_classes, pretrained=False, model_path=None):\n",
    "        super(ViTBase16, self).__init__()\n",
    "\n",
    "        # Crear el modelo Vision Transformer\n",
    "        self.model = timm.create_model(\"vit_base_patch16_224\", pretrained=False)\n",
    "\n",
    "        # Cargar pesos preentrenados si se solicita\n",
    "        if pretrained:\n",
    "            if model_path is not None:\n",
    "                self.model.load_state_dict(torch.load(model_path))\n",
    "            else:\n",
    "                raise ValueError(\"Se requiere un modelo preentrenado y una ruta de archivo si se activa `pretrained`.\")\n",
    "\n",
    "        # Ajustar la última capa para que tenga el número correcto de clases\n",
    "        self.model.head = nn.Linear(self.model.head.in_features, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pasar las imágenes por el modelo para obtener las predicciones\n",
    "        return self.model(x)\n",
    "\n",
    "    def train_one_epoch(self, train_loader, criterion, optimizer, device, lr_scheduler=None):\n",
    "        # Inicializar variables de pérdida y precisión\n",
    "        epoch_loss = 0.0\n",
    "        epoch_accuracy = 0.0\n",
    "\n",
    "        # Establecer el modelo en modo de entrenamiento\n",
    "        self.model.train()\n",
    "\n",
    "        for i, (data, target) in enumerate(train_loader):\n",
    "            # Enviar los datos al dispositivo adecuado (CPU o GPU)\n",
    "            data, target = data.to(device), target.to(device)\n",
    "\n",
    "            # Limpiar los gradientes del optimizador\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Paso hacia adelante: calcular las predicciones del modelo\n",
    "            output = self.forward(data)\n",
    "\n",
    "            # Calcular la pérdida\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "            # Paso hacia atrás: calcular los gradientes de la pérdida con respecto a los parámetros del modelo\n",
    "            loss.backward()\n",
    "\n",
    "            # Calcular la precisión\n",
    "            accuracy = (output.argmax(dim=1) == target).float().mean()\n",
    "\n",
    "            # Acumular la pérdida y precisión de la época\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_accuracy += accuracy.item()\n",
    "\n",
    "            # Realizar un paso de optimización\n",
    "            optimizer.step()\n",
    "\n",
    "            # Actualizar la tasa de aprendizaje si se usa un scheduler\n",
    "            if lr_scheduler:\n",
    "                lr_scheduler.step()\n",
    "\n",
    "            # Imprimir información sobre el progreso\n",
    "            if i % 20 == 0:\n",
    "                print(f\"\\tBATCH {i+1}/{len(train_loader)} - LOSS: {loss.item():.4f} - ACC: {accuracy.item():.4f}\")\n",
    "\n",
    "        # Promediar la pérdida y precisión para la época\n",
    "        return epoch_loss / len(train_loader), epoch_accuracy / len(train_loader)\n",
    "\n",
    "    def validate_one_epoch(self, valid_loader, criterion, device):\n",
    "        # Inicializar variables para la pérdida y precisión de validación\n",
    "        valid_loss = 0.0\n",
    "        valid_accuracy = 0.0\n",
    "\n",
    "        # Establecer el modelo en modo de evaluación (sin gradientes)\n",
    "        self.model.eval()\n",
    "\n",
    "        with torch.no_grad():  # Desactivar el cálculo de gradientes para la validación\n",
    "            for data, target in valid_loader:\n",
    "                # Enviar los datos al dispositivo adecuado\n",
    "                data, target = data.to(device), target.to(device)\n",
    "    \n",
    "                # Paso hacia adelante: calcular las predicciones\n",
    "                output = self.model(data)\n",
    "    \n",
    "                # Calcular la pérdida\n",
    "                loss = criterion(output, target)\n",
    "    \n",
    "                # Calcular la precisión\n",
    "                accuracy = (output.argmax(dim=1) == target).float().mean()\n",
    "    \n",
    "                # Acumular la pérdida y precisión de validación\n",
    "                valid_loss += loss.item()\n",
    "                valid_accuracy += accuracy.item()\n",
    "\n",
    "        # Promediar la pérdida y precisión para la validación\n",
    "        return valid_loss / len(valid_loader), valid_accuracy / len(valid_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `fit_tpu`\n",
    "\n",
    "El código a continuación define una función llamada `fit_tpu`, que entrena y valida un modelo en una arquitectura distribuida utilizando TPUs. Esta función maneja tanto el proceso de entrenamiento como el de validación, e implementa técnicas de optimización para mejorar la eficiencia durante el entrenamiento en entornos distribuidos.\n",
    "\n",
    "1. **Inicialización de Variables**:\n",
    "   - Se inicializan las listas `train_losses`, `valid_losses`, `train_accs`, y `valid_accs` para almacenar las métricas de pérdida y precisión durante cada época.\n",
    "   - Se establece la variable `valid_loss_min` como `np.inf`, que se usará para hacer un seguimiento de la menor pérdida de validación observada.\n",
    "\n",
    "2. **Ciclo de Entrenamiento**:\n",
    "   - La función realiza un ciclo de entrenamiento que se repite por el número de épocas especificado (`epochs`).\n",
    "   - En cada época, se realiza lo siguiente:\n",
    "     - **Entrenamiento**: Utiliza el método `train_one_epoch` para entrenar el modelo en los datos de entrenamiento (`train_loader`). Durante el entrenamiento, se calcula la pérdida y la precisión, y se imprime esta información para cada época.\n",
    "     - **Validación**: Si se proporciona un conjunto de datos de validación (`valid_loader`), se realiza la validación utilizando el método `validate_one_epoch`. Durante la validación, se calcula la pérdida y la precisión, y se imprime esta información.\n",
    "   \n",
    "3. **Manejo de Memoria**:\n",
    "   - Se usa `gc.collect()` después de cada operación importante para liberar la memoria no utilizada y optimizar el uso de recursos, especialmente cuando se entrena en entornos distribuidos como TPUs.\n",
    "\n",
    "4. **Guardado del Mejor Modelo**:\n",
    "   - Si la pérdida de validación mejora (es decir, es menor que la pérdida mínima registrada), se guarda el modelo actual. Esto permite mantener la mejor versión del modelo durante el entrenamiento.\n",
    "\n",
    "5. **Devolución de Resultados**:\n",
    "   - Al final de las épocas, la función devuelve un diccionario que contiene las pérdidas y precisiones de entrenamiento y validación para cada época. Esto permite hacer un seguimiento detallado del rendimiento del modelo a lo largo del tiempo.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-22T10:58:14.337883Z",
     "iopub.status.busy": "2025-07-22T10:58:14.337656Z",
     "iopub.status.idle": "2025-07-22T10:58:14.353266Z",
     "shell.execute_reply": "2025-07-22T10:58:14.348009Z",
     "shell.execute_reply.started": "2025-07-22T10:58:14.337862Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def fit_tpu(model, epochs, device, criterion, optimizer, train_loader, valid_loader=None):\n",
    "    import torch_xla.core.xla_model as xm\n",
    "    import torch_xla.distributed.parallel_loader as pl\n",
    "    \n",
    "    valid_loss_min = np.inf\n",
    "    train_losses, valid_losses = [], []\n",
    "    train_accs, valid_accs = [], []\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        gc.collect()\n",
    "        para_train_loader = pl.ParallelLoader(train_loader, [device])\n",
    "\n",
    "        xm.master_print(f\"{'='*50}\")\n",
    "        xm.master_print(f\"EPOCH {epoch} - TRAINING...\")\n",
    "\n",
    "        train_loss, train_acc = model.train_one_epoch(\n",
    "            para_train_loader.per_device_loader(device), criterion, optimizer, device\n",
    "        )\n",
    "        \n",
    "        xm.master_print(f\"\\n\\t[TRAIN] EPOCH {epoch} - LOSS: {train_loss:.4f}, ACCURACY: {train_acc:.4f}\\n\")\n",
    "        train_losses.append(train_loss)\n",
    "        train_accs.append(train_acc)\n",
    "        \n",
    "        if valid_loader is not None:\n",
    "            gc.collect()\n",
    "            para_valid_loader = pl.ParallelLoader(valid_loader, [device])\n",
    "            xm.master_print(f\"EPOCH {epoch} - VALIDATING...\")\n",
    "\n",
    "            with torch.no_grad():\n",
    "                valid_loss, valid_acc = model.validate_one_epoch(\n",
    "                    para_valid_loader.per_device_loader(device), criterion, device\n",
    "                )\n",
    "                \n",
    "            xm.master_print(f\"\\t[VALID] LOSS: {valid_loss:.4f}, ACCURACY: {valid_acc:.4f}\\n\")\n",
    "            valid_losses.append(valid_loss)\n",
    "            valid_accs.append(valid_acc)\n",
    "\n",
    "            if valid_loss < valid_loss_min:\n",
    "                xm.master_print(f\"Validation loss improved ({valid_loss_min:.4f} --> {valid_loss:.4f}). Saving model...\")\n",
    "                xm.save(model.state_dict(), f'best_model_epoch_{epoch}.pth')\n",
    "                valid_loss_min = valid_loss\n",
    "\n",
    "        xm.mark_step()\n",
    "        gc.collect()\n",
    "\n",
    "    return {\n",
    "        \"train_loss\": train_losses,\n",
    "        \"valid_loss\": valid_losses,\n",
    "        \"train_acc\": train_accs,\n",
    "        \"valid_acc\": valid_accs,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `_run`\n",
    "\n",
    "El código a continuación define la función `_run`, que se encarga de configurar y ejecutar el entrenamiento del modelo Vision Transformer (ViT) en un entorno distribuido utilizando TPUs (Tensor Processing Units). Esta función realiza varias tareas importantes, como la preparación de los datos, la inicialización del modelo, el entrenamiento y la validación, y finalmente guarda el modelo entrenado.\n",
    "\n",
    "1. **Cargar los Conjuntos de Datos**:\n",
    "   - Los datasets de entrenamiento (`train_df`) y validación (`valid_df`) se cargan utilizando la clase `CassavaDataset`, con las transformaciones especificadas en `transforms_train` y `transforms_valid`, respectivamente.\n",
    "\n",
    "2. **Samplers Distribuidos**:\n",
    "   - Se utilizan **samplers distribuidos** para asegurar que el entrenamiento y la validación se distribuyan correctamente entre las distintas TPUs. Esto es esencial cuando se entrena el modelo en varios dispositivos en paralelo.\n",
    "   - Los samplers distribuidos aseguran que cada TPU procese solo una parte del conjunto de datos.\n",
    "\n",
    "3. **DataLoader**:\n",
    "   - Se crean los **DataLoaders** para cargar los datos de entrenamiento y validación en lotes. Estos se configuran con un tamaño de lote (`BATCH_SIZE`), utilizando los samplers para distribuir los datos y especificando el número de workers para la carga paralela de los datos.\n",
    "\n",
    "4. **Inicialización del Modelo**:\n",
    "   - La función de pérdida `CrossEntropyLoss` se utiliza, que es comúnmente utilizada para tareas de clasificación.\n",
    "   - El dispositivo (TPU) se define utilizando `xm.xla_device()`, asegurando que el modelo se ejecute en TPUs en lugar de GPUs o CPUs.\n",
    "\n",
    "5. **Configuración del Optimizer y la Tasa de Aprendizaje**:\n",
    "   - Se configura un optimizador **Adam** con una tasa de aprendizaje ajustada según el número de TPUs. Esto es importante cuando se entrena en un entorno distribuido para garantizar que la tasa de aprendizaje sea adecuada.\n",
    "\n",
    "6. **Entrenamiento y Validación**:\n",
    "   - El entrenamiento se realiza utilizando la función `fit_tpu`, que se encarga de ejecutar el ciclo de entrenamiento y validación. Durante el entrenamiento, se calculan las métricas de pérdida y precisión y se imprimen para monitorear el progreso.\n",
    "   - Después de cada época de validación, se guarda el modelo si la pérdida de validación ha disminuido, asegurando que el modelo con el mejor rendimiento se conserve.\n",
    "\n",
    "7. **Guardado del Modelo**:\n",
    "   - Al final del entrenamiento, el modelo entrenado se guarda en un archivo con un nombre que incluye la fecha y hora actual para facilitar la organización de los modelos entrenados.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-22T10:58:14.356218Z",
     "iopub.status.busy": "2025-07-22T10:58:14.355398Z",
     "iopub.status.idle": "2025-07-22T10:58:14.372776Z",
     "shell.execute_reply": "2025-07-22T10:58:14.367428Z",
     "shell.execute_reply.started": "2025-07-22T10:58:14.356191Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def _run():\n",
    "    import torch_xla.core.xla_model as xm\n",
    "    \n",
    "    try:\n",
    "        # Inicializar el dispositivo TPU con una mejor gestión de errores\n",
    "        device = xm.xla_device()\n",
    "        xm.master_print(f\"Successfully initialized TPU device: {device}\")\n",
    "        xm.master_print(f\"World size: {xm.xrt_world_size()}\")\n",
    "        xm.master_print(f\"Local rank: {xm.get_ordinal()}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Failed to initialize TPU: {e}\")\n",
    "        print(\"Falling back to CPU...\")\n",
    "        device = torch.device('cpu')\n",
    "    \n",
    "    # Crear el modelo\n",
    "    model = ViTBase16(n_classes=5, pretrained=False)\n",
    "    model.to(device)\n",
    "    \n",
    "    # crear los datasets\n",
    "    train_dataset = CassavaDataset(train_df, transforms=transforms_train)\n",
    "    valid_dataset = CassavaDataset(valid_df, transforms=transforms_valid)\n",
    "\n",
    "    # Comprobar si estamos utilizando TPU formación distribuida\n",
    "    if str(device).startswith('xla'):\n",
    "        train_sampler = DistributedSampler(\n",
    "            train_dataset,\n",
    "            num_replicas=xm.xrt_world_size(),\n",
    "            rank=xm.get_ordinal(),\n",
    "            shuffle=True,\n",
    "        )\n",
    "\n",
    "        valid_sampler = DistributedSampler(\n",
    "            valid_dataset,\n",
    "            num_replicas=xm.xrt_world_size(),\n",
    "            rank=xm.get_ordinal(),\n",
    "            shuffle=False,\n",
    "        )\n",
    "        \n",
    "        lr_multiplier = xm.xrt_world_size()\n",
    "    else:\n",
    "        train_sampler = None\n",
    "        valid_sampler = None\n",
    "        lr_multiplier = 1\n",
    "\n",
    "    # DataLoaders\n",
    "    train_loader = DataLoader(\n",
    "        dataset=train_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        sampler=train_sampler,\n",
    "        shuffle=(train_sampler is None),\n",
    "        drop_last=True,\n",
    "        num_workers=2,  \n",
    "    )\n",
    "\n",
    "    valid_loader = DataLoader(\n",
    "        dataset=valid_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        sampler=valid_sampler,\n",
    "        shuffle=False,\n",
    "        drop_last=True,\n",
    "        num_workers=2,\n",
    "    )\n",
    "\n",
    "    # Optimizador y criterios\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    lr = LR * lr_multiplier\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=0.01)\n",
    "\n",
    "    # Informacion de entrenamiento\n",
    "    print(f\"Device: {device}\")\n",
    "    print(f\"Learning Rate: {lr}\")\n",
    "    start_time = datetime.now()\n",
    "    print(f\"Start Time: {start_time}\")\n",
    "\n",
    "    # Entrenar\n",
    "    logs = fit_tpu(\n",
    "        model=model,\n",
    "        epochs=N_EPOCHS,\n",
    "        device=device,\n",
    "        criterion=criterion,\n",
    "        optimizer=optimizer,\n",
    "        train_loader=train_loader,\n",
    "        valid_loader=valid_loader,\n",
    "    )\n",
    "\n",
    "    # Guardar modelo final\n",
    "    end_time = datetime.now()\n",
    "    print(f\"Training completed in: {end_time - start_time}\")\n",
    "    \n",
    "    final_model_path = f'model_final_{end_time.strftime(\"%Y%m%d_%H%M\")}.pth'\n",
    "    if str(device).startswith('xla'):\n",
    "        xm.save(model.state_dict(), final_model_path)\n",
    "        xm.master_print(f\"Final model saved as: {final_model_path}\")\n",
    "    else:\n",
    "        torch.save(model.state_dict(), final_model_path)\n",
    "        print(f\"Final model saved as: {final_model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-22T10:58:14.374673Z",
     "iopub.status.busy": "2025-07-22T10:58:14.374427Z",
     "iopub.status.idle": "2025-07-22T13:33:52.122723Z",
     "shell.execute_reply": "2025-07-22T13:33:52.117244Z",
     "shell.execute_reply.started": "2025-07-22T10:58:14.374651Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting TPU training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/torch_xla/__init__.py:251: UserWarning: `tensorflow` can conflict with `torch-xla`. Prefer `tensorflow-cpu` when using PyTorch/XLA. To silence this warning, `pip uninstall -y tensorflow && pip install tensorflow-cpu`. If you are in a notebook environment such as Colab or Kaggle, restart your notebook runtime afterwards.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting TPU multiprocessing training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before InitGoogle() is written to STDERR\n",
      "WARNING: Logging before InitGoogle() is written to STDERR\n",
      "E0000 00:00:1753181901.529633     364 common_lib.cc:621] Could not set metric server port: INVALID_ARGUMENT: Could not find SliceBuilder port 8476 in any of the 0 ports provided in `tpu_process_addresses`=\"local\"\n",
      "=== Source Location Trace: ===\n",
      "learning/45eac/tfrc/runtime/common_lib.cc:232\n",
      "E0000 00:00:1753181901.529645     365 common_lib.cc:621] Could not set metric server port: INVALID_ARGUMENT: Could not find SliceBuilder port 8477 in any of the 0 ports provided in `tpu_process_addresses`=\"local\"\n",
      "=== Source Location Trace: === \n",
      "learning/45eac/tfrc/runtime/common_lib.cc:232\n",
      "WARNING: Logging before InitGoogle() is written to STDERR\n",
      "E0000 00:00:1753181901.529671     367 common_lib.cc:621] Could not set metric server port: INVALID_ARGUMENT: Could not find SliceBuilder port 8479 in any of the 0 ports provided in `tpu_process_addresses`=\"local\"\n",
      "=== Source Location Trace: ===\n",
      "learning/45eac/tfrc/runtime/common_lib.cc:232\n",
      "WARNING: Logging before InitGoogle() is written to STDERR\n",
      "E0000 00:00:1753181901.537640     366 common_lib.cc:621] Could not set metric server port: INVALID_ARGUMENT: Could not find SliceBuilder port 8478 in any of the 0 ports provided in `tpu_process_addresses`=\"local\"\n",
      "=== Source Location Trace: ===\n",
      "learning/45eac/tfrc/runtime/common_lib.cc:232\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TPU multiprocessing failed: Bad StatusOr access: UNKNOWN: TPU initialization failed: Invalid --2a886c8_slice_builder_worker_addresses specified. Expected 4 worker addresses, got 1.\n",
      "Trying single TPU core...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before InitGoogle() is written to STDERR\n",
      "E0000 00:00:1753181901.696667      10 common_lib.cc:621] Could not set metric server port: INVALID_ARGUMENT: Could not find SliceBuilder port 8471 in any of the 0 ports provided in `tpu_process_addresses`=\"local\"\n",
      "=== Source Location Trace: === \n",
      "learning/45eac/tfrc/runtime/common_lib.cc:232\n",
      "WARNING:root:torch_xla.core.xla_model.xrt_world_size() will be removed in release 2.7. is deprecated. Use torch_xla.runtime.world_size instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully initialized TPU device: xla:0\n",
      "World size: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:torch_xla.core.xla_model.xla_model.get_ordinal() will be removed in release 2.7. is deprecated. Use torch_xla.runtime.global_ordinal instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local rank: 0\n",
      "Device: xla:0\n",
      "Learning Rate: 2e-05\n",
      "Start Time: 2025-07-22 10:58:27.333753\n",
      "==================================================\n",
      "EPOCH 1 - TRAINING...\n",
      "\tBATCH 1/1203 - LOSS: 1.9688 - ACC: 0.2500\n",
      "\tBATCH 21/1203 - LOSS: 1.0547 - ACC: 0.7500\n",
      "\tBATCH 41/1203 - LOSS: 1.3047 - ACC: 0.5000\n",
      "\tBATCH 61/1203 - LOSS: 1.1328 - ACC: 0.6250\n",
      "\tBATCH 81/1203 - LOSS: 1.3203 - ACC: 0.4375\n",
      "\tBATCH 101/1203 - LOSS: 1.3906 - ACC: 0.4375\n",
      "\tBATCH 121/1203 - LOSS: 1.4609 - ACC: 0.4375\n",
      "\tBATCH 141/1203 - LOSS: 0.8594 - ACC: 0.7500\n",
      "\tBATCH 161/1203 - LOSS: 1.3047 - ACC: 0.5000\n",
      "\tBATCH 181/1203 - LOSS: 1.0078 - ACC: 0.6875\n",
      "\tBATCH 201/1203 - LOSS: 1.2969 - ACC: 0.5625\n",
      "\tBATCH 221/1203 - LOSS: 0.9844 - ACC: 0.6875\n",
      "\tBATCH 241/1203 - LOSS: 1.4141 - ACC: 0.5625\n",
      "\tBATCH 261/1203 - LOSS: 0.8945 - ACC: 0.7500\n",
      "\tBATCH 281/1203 - LOSS: 1.2812 - ACC: 0.5625\n",
      "\tBATCH 301/1203 - LOSS: 1.1719 - ACC: 0.6250\n",
      "\tBATCH 321/1203 - LOSS: 1.0312 - ACC: 0.7500\n",
      "\tBATCH 341/1203 - LOSS: 0.9414 - ACC: 0.6875\n",
      "\tBATCH 361/1203 - LOSS: 1.3984 - ACC: 0.5000\n",
      "\tBATCH 381/1203 - LOSS: 1.3828 - ACC: 0.5000\n",
      "\tBATCH 401/1203 - LOSS: 1.0938 - ACC: 0.6250\n",
      "\tBATCH 421/1203 - LOSS: 1.3984 - ACC: 0.6250\n",
      "\tBATCH 441/1203 - LOSS: 1.2812 - ACC: 0.5000\n",
      "\tBATCH 461/1203 - LOSS: 1.2500 - ACC: 0.5000\n",
      "\tBATCH 481/1203 - LOSS: 1.1484 - ACC: 0.6250\n",
      "\tBATCH 501/1203 - LOSS: 1.0781 - ACC: 0.5000\n",
      "\tBATCH 521/1203 - LOSS: 1.2969 - ACC: 0.5625\n",
      "\tBATCH 541/1203 - LOSS: 1.3438 - ACC: 0.4375\n",
      "\tBATCH 561/1203 - LOSS: 1.1172 - ACC: 0.5625\n",
      "\tBATCH 581/1203 - LOSS: 1.3516 - ACC: 0.4375\n",
      "\tBATCH 601/1203 - LOSS: 1.0000 - ACC: 0.6250\n",
      "\tBATCH 621/1203 - LOSS: 1.1094 - ACC: 0.6875\n",
      "\tBATCH 641/1203 - LOSS: 1.1094 - ACC: 0.6875\n",
      "\tBATCH 661/1203 - LOSS: 1.0391 - ACC: 0.6250\n",
      "\tBATCH 681/1203 - LOSS: 0.8633 - ACC: 0.7500\n",
      "\tBATCH 701/1203 - LOSS: 1.2266 - ACC: 0.5625\n",
      "\tBATCH 721/1203 - LOSS: 0.6758 - ACC: 0.8125\n",
      "\tBATCH 741/1203 - LOSS: 1.6484 - ACC: 0.3125\n",
      "\tBATCH 761/1203 - LOSS: 0.8633 - ACC: 0.6875\n",
      "\tBATCH 781/1203 - LOSS: 0.6406 - ACC: 0.8125\n",
      "\tBATCH 801/1203 - LOSS: 1.2188 - ACC: 0.5625\n",
      "\tBATCH 821/1203 - LOSS: 1.1641 - ACC: 0.6250\n",
      "\tBATCH 841/1203 - LOSS: 1.0234 - ACC: 0.6250\n",
      "\tBATCH 861/1203 - LOSS: 1.8203 - ACC: 0.3750\n",
      "\tBATCH 881/1203 - LOSS: 1.5078 - ACC: 0.4375\n",
      "\tBATCH 901/1203 - LOSS: 0.8945 - ACC: 0.7500\n",
      "\tBATCH 921/1203 - LOSS: 0.9883 - ACC: 0.7500\n",
      "\tBATCH 941/1203 - LOSS: 1.2891 - ACC: 0.5625\n",
      "\tBATCH 961/1203 - LOSS: 1.1328 - ACC: 0.5000\n",
      "\tBATCH 981/1203 - LOSS: 1.5000 - ACC: 0.4375\n",
      "\tBATCH 1001/1203 - LOSS: 1.2578 - ACC: 0.5625\n",
      "\tBATCH 1021/1203 - LOSS: 1.4531 - ACC: 0.5000\n",
      "\tBATCH 1041/1203 - LOSS: 1.5000 - ACC: 0.3750\n",
      "\tBATCH 1061/1203 - LOSS: 1.2891 - ACC: 0.5625\n",
      "\tBATCH 1081/1203 - LOSS: 0.9492 - ACC: 0.6875\n",
      "\tBATCH 1101/1203 - LOSS: 1.0547 - ACC: 0.6250\n",
      "\tBATCH 1121/1203 - LOSS: 1.1562 - ACC: 0.5625\n",
      "\tBATCH 1141/1203 - LOSS: 1.2344 - ACC: 0.5625\n",
      "\tBATCH 1161/1203 - LOSS: 0.6680 - ACC: 0.8125\n",
      "\tBATCH 1181/1203 - LOSS: 1.0156 - ACC: 0.6250\n",
      "\tBATCH 1201/1203 - LOSS: 1.4297 - ACC: 0.4375\n",
      "\n",
      "\t[TRAIN] EPOCH 1 - LOSS: 1.1340, ACCURACY: 0.6176\n",
      "\n",
      "EPOCH 1 - VALIDATING...\n",
      "\t[VALID] LOSS: 1.0919, ACCURACY: 0.6217\n",
      "\n",
      "Validation loss improved (inf --> 1.0919). Saving model...\n",
      "==================================================\n",
      "EPOCH 2 - TRAINING...\n",
      "\tBATCH 1/1203 - LOSS: 1.0078 - ACC: 0.6250\n",
      "\tBATCH 21/1203 - LOSS: 0.9258 - ACC: 0.7500\n",
      "\tBATCH 41/1203 - LOSS: 1.0703 - ACC: 0.5625\n",
      "\tBATCH 61/1203 - LOSS: 1.0312 - ACC: 0.6250\n",
      "\tBATCH 81/1203 - LOSS: 1.1484 - ACC: 0.6250\n",
      "\tBATCH 101/1203 - LOSS: 1.3359 - ACC: 0.4375\n",
      "\tBATCH 121/1203 - LOSS: 1.4375 - ACC: 0.3750\n",
      "\tBATCH 141/1203 - LOSS: 0.7188 - ACC: 0.7500\n",
      "\tBATCH 161/1203 - LOSS: 1.2109 - ACC: 0.5000\n",
      "\tBATCH 181/1203 - LOSS: 0.8516 - ACC: 0.7500\n",
      "\tBATCH 201/1203 - LOSS: 1.2656 - ACC: 0.5000\n",
      "\tBATCH 221/1203 - LOSS: 0.7812 - ACC: 0.7500\n",
      "\tBATCH 241/1203 - LOSS: 1.1484 - ACC: 0.6250\n",
      "\tBATCH 261/1203 - LOSS: 0.9688 - ACC: 0.6875\n",
      "\tBATCH 281/1203 - LOSS: 1.2188 - ACC: 0.6250\n",
      "\tBATCH 301/1203 - LOSS: 1.1719 - ACC: 0.5625\n",
      "\tBATCH 321/1203 - LOSS: 0.8008 - ACC: 0.7500\n",
      "\tBATCH 341/1203 - LOSS: 0.6992 - ACC: 0.8125\n",
      "\tBATCH 361/1203 - LOSS: 1.2734 - ACC: 0.5000\n",
      "\tBATCH 381/1203 - LOSS: 1.0312 - ACC: 0.6250\n",
      "\tBATCH 401/1203 - LOSS: 1.0234 - ACC: 0.6250\n",
      "\tBATCH 421/1203 - LOSS: 1.3359 - ACC: 0.6250\n",
      "\tBATCH 441/1203 - LOSS: 1.1016 - ACC: 0.5000\n",
      "\tBATCH 461/1203 - LOSS: 1.0859 - ACC: 0.5625\n",
      "\tBATCH 481/1203 - LOSS: 0.9219 - ACC: 0.6250\n",
      "\tBATCH 501/1203 - LOSS: 0.9805 - ACC: 0.5625\n",
      "\tBATCH 521/1203 - LOSS: 1.1562 - ACC: 0.6250\n",
      "\tBATCH 541/1203 - LOSS: 1.3750 - ACC: 0.4375\n",
      "\tBATCH 561/1203 - LOSS: 0.9922 - ACC: 0.6250\n",
      "\tBATCH 581/1203 - LOSS: 1.2891 - ACC: 0.5000\n",
      "\tBATCH 601/1203 - LOSS: 0.9414 - ACC: 0.5625\n",
      "\tBATCH 621/1203 - LOSS: 1.0078 - ACC: 0.6875\n",
      "\tBATCH 641/1203 - LOSS: 1.1953 - ACC: 0.5000\n",
      "\tBATCH 661/1203 - LOSS: 0.9648 - ACC: 0.6875\n",
      "\tBATCH 681/1203 - LOSS: 0.7812 - ACC: 0.7500\n",
      "\tBATCH 701/1203 - LOSS: 1.1484 - ACC: 0.5625\n",
      "\tBATCH 721/1203 - LOSS: 0.5664 - ACC: 0.8750\n",
      "\tBATCH 741/1203 - LOSS: 1.4062 - ACC: 0.3750\n",
      "\tBATCH 761/1203 - LOSS: 0.8906 - ACC: 0.6250\n",
      "\tBATCH 781/1203 - LOSS: 0.5781 - ACC: 0.8750\n",
      "\tBATCH 801/1203 - LOSS: 1.2031 - ACC: 0.5000\n",
      "\tBATCH 821/1203 - LOSS: 1.0234 - ACC: 0.6875\n",
      "\tBATCH 841/1203 - LOSS: 0.9219 - ACC: 0.6875\n",
      "\tBATCH 861/1203 - LOSS: 1.7344 - ACC: 0.3750\n",
      "\tBATCH 881/1203 - LOSS: 1.2500 - ACC: 0.4375\n",
      "\tBATCH 901/1203 - LOSS: 0.7539 - ACC: 0.8125\n",
      "\tBATCH 921/1203 - LOSS: 0.7773 - ACC: 0.7500\n",
      "\tBATCH 941/1203 - LOSS: 1.2109 - ACC: 0.5625\n",
      "\tBATCH 961/1203 - LOSS: 1.1250 - ACC: 0.5000\n",
      "\tBATCH 981/1203 - LOSS: 1.4141 - ACC: 0.4375\n",
      "\tBATCH 1001/1203 - LOSS: 0.9922 - ACC: 0.5625\n",
      "\tBATCH 1021/1203 - LOSS: 1.3672 - ACC: 0.5000\n",
      "\tBATCH 1041/1203 - LOSS: 1.4219 - ACC: 0.3750\n",
      "\tBATCH 1061/1203 - LOSS: 1.3359 - ACC: 0.4375\n",
      "\tBATCH 1081/1203 - LOSS: 0.9180 - ACC: 0.7500\n",
      "\tBATCH 1101/1203 - LOSS: 1.1875 - ACC: 0.6875\n",
      "\tBATCH 1121/1203 - LOSS: 1.1250 - ACC: 0.5625\n",
      "\tBATCH 1141/1203 - LOSS: 1.1406 - ACC: 0.5625\n",
      "\tBATCH 1161/1203 - LOSS: 0.5703 - ACC: 0.8125\n",
      "\tBATCH 1181/1203 - LOSS: 0.9688 - ACC: 0.6250\n",
      "\tBATCH 1201/1203 - LOSS: 1.3281 - ACC: 0.5000\n",
      "\n",
      "\t[TRAIN] EPOCH 2 - LOSS: 1.0457, ACCURACY: 0.6313\n",
      "\n",
      "EPOCH 2 - VALIDATING...\n",
      "\t[VALID] LOSS: 1.0488, ACCURACY: 0.6320\n",
      "\n",
      "Validation loss improved (1.0919 --> 1.0488). Saving model...\n",
      "==================================================\n",
      "EPOCH 3 - TRAINING...\n",
      "\tBATCH 1/1203 - LOSS: 0.8945 - ACC: 0.6250\n",
      "\tBATCH 21/1203 - LOSS: 0.8828 - ACC: 0.7500\n",
      "\tBATCH 41/1203 - LOSS: 1.0781 - ACC: 0.5000\n",
      "\tBATCH 61/1203 - LOSS: 0.9180 - ACC: 0.6250\n",
      "\tBATCH 81/1203 - LOSS: 1.1641 - ACC: 0.6250\n",
      "\tBATCH 101/1203 - LOSS: 1.2109 - ACC: 0.5625\n",
      "\tBATCH 121/1203 - LOSS: 1.3281 - ACC: 0.4375\n",
      "\tBATCH 141/1203 - LOSS: 0.7305 - ACC: 0.8125\n",
      "\tBATCH 161/1203 - LOSS: 1.1016 - ACC: 0.5000\n",
      "\tBATCH 181/1203 - LOSS: 0.8867 - ACC: 0.6250\n",
      "\tBATCH 201/1203 - LOSS: 1.1719 - ACC: 0.5625\n",
      "\tBATCH 221/1203 - LOSS: 0.7578 - ACC: 0.6875\n",
      "\tBATCH 241/1203 - LOSS: 1.0703 - ACC: 0.5625\n",
      "\tBATCH 261/1203 - LOSS: 0.9180 - ACC: 0.6875\n",
      "\tBATCH 281/1203 - LOSS: 1.2031 - ACC: 0.6250\n",
      "\tBATCH 301/1203 - LOSS: 1.1406 - ACC: 0.5625\n",
      "\tBATCH 321/1203 - LOSS: 0.7891 - ACC: 0.7500\n",
      "\tBATCH 341/1203 - LOSS: 0.6523 - ACC: 0.8750\n",
      "\tBATCH 361/1203 - LOSS: 1.3281 - ACC: 0.5000\n",
      "\tBATCH 381/1203 - LOSS: 0.9766 - ACC: 0.6250\n",
      "\tBATCH 401/1203 - LOSS: 0.9961 - ACC: 0.6875\n",
      "\tBATCH 421/1203 - LOSS: 1.2578 - ACC: 0.6250\n",
      "\tBATCH 441/1203 - LOSS: 1.2734 - ACC: 0.3750\n",
      "\tBATCH 461/1203 - LOSS: 1.1484 - ACC: 0.5000\n",
      "\tBATCH 481/1203 - LOSS: 1.0156 - ACC: 0.5625\n",
      "\tBATCH 501/1203 - LOSS: 0.8711 - ACC: 0.5625\n",
      "\tBATCH 521/1203 - LOSS: 1.0391 - ACC: 0.6250\n",
      "\tBATCH 541/1203 - LOSS: 0.9922 - ACC: 0.5625\n",
      "\tBATCH 561/1203 - LOSS: 1.0859 - ACC: 0.5625\n",
      "\tBATCH 581/1203 - LOSS: 1.4609 - ACC: 0.5000\n",
      "\tBATCH 601/1203 - LOSS: 0.9844 - ACC: 0.5625\n",
      "\tBATCH 621/1203 - LOSS: 0.8359 - ACC: 0.6875\n",
      "\tBATCH 641/1203 - LOSS: 1.2500 - ACC: 0.5625\n",
      "\tBATCH 661/1203 - LOSS: 0.9219 - ACC: 0.7500\n",
      "\tBATCH 681/1203 - LOSS: 0.8242 - ACC: 0.8125\n",
      "\tBATCH 701/1203 - LOSS: 1.0156 - ACC: 0.6875\n",
      "\tBATCH 721/1203 - LOSS: 0.5273 - ACC: 0.8750\n",
      "\tBATCH 741/1203 - LOSS: 1.4688 - ACC: 0.3125\n",
      "\tBATCH 761/1203 - LOSS: 0.9688 - ACC: 0.6875\n",
      "\tBATCH 781/1203 - LOSS: 0.6289 - ACC: 0.8125\n",
      "\tBATCH 801/1203 - LOSS: 1.1641 - ACC: 0.5000\n",
      "\tBATCH 821/1203 - LOSS: 1.0078 - ACC: 0.6250\n",
      "\tBATCH 841/1203 - LOSS: 0.9727 - ACC: 0.5625\n",
      "\tBATCH 861/1203 - LOSS: 1.5781 - ACC: 0.5000\n",
      "\tBATCH 881/1203 - LOSS: 1.0859 - ACC: 0.5625\n",
      "\tBATCH 901/1203 - LOSS: 0.7773 - ACC: 0.8750\n",
      "\tBATCH 921/1203 - LOSS: 0.6719 - ACC: 0.8125\n",
      "\tBATCH 941/1203 - LOSS: 1.3516 - ACC: 0.5625\n",
      "\tBATCH 961/1203 - LOSS: 1.0312 - ACC: 0.5625\n",
      "\tBATCH 981/1203 - LOSS: 1.2188 - ACC: 0.5000\n",
      "\tBATCH 1001/1203 - LOSS: 1.0156 - ACC: 0.6250\n",
      "\tBATCH 1021/1203 - LOSS: 1.4766 - ACC: 0.5000\n",
      "\tBATCH 1041/1203 - LOSS: 1.2656 - ACC: 0.3750\n",
      "\tBATCH 1061/1203 - LOSS: 1.1875 - ACC: 0.5000\n",
      "\tBATCH 1081/1203 - LOSS: 0.9180 - ACC: 0.6875\n",
      "\tBATCH 1101/1203 - LOSS: 1.1172 - ACC: 0.6250\n",
      "\tBATCH 1121/1203 - LOSS: 1.1016 - ACC: 0.5625\n",
      "\tBATCH 1141/1203 - LOSS: 1.1406 - ACC: 0.5625\n",
      "\tBATCH 1161/1203 - LOSS: 0.5664 - ACC: 0.8125\n",
      "\tBATCH 1181/1203 - LOSS: 0.9336 - ACC: 0.6875\n",
      "\tBATCH 1201/1203 - LOSS: 1.3828 - ACC: 0.4375\n",
      "\n",
      "\t[TRAIN] EPOCH 3 - LOSS: 1.0085, ACCURACY: 0.6378\n",
      "\n",
      "EPOCH 3 - VALIDATING...\n",
      "\t[VALID] LOSS: 1.0226, ACCURACY: 0.6419\n",
      "\n",
      "Validation loss improved (1.0488 --> 1.0226). Saving model...\n",
      "==================================================\n",
      "EPOCH 4 - TRAINING...\n",
      "\tBATCH 1/1203 - LOSS: 0.9844 - ACC: 0.6875\n",
      "\tBATCH 21/1203 - LOSS: 0.8633 - ACC: 0.7500\n",
      "\tBATCH 41/1203 - LOSS: 1.0859 - ACC: 0.6250\n",
      "\tBATCH 61/1203 - LOSS: 0.8672 - ACC: 0.6250\n",
      "\tBATCH 81/1203 - LOSS: 1.0859 - ACC: 0.5625\n",
      "\tBATCH 101/1203 - LOSS: 1.2109 - ACC: 0.6250\n",
      "\tBATCH 121/1203 - LOSS: 1.4297 - ACC: 0.4375\n",
      "\tBATCH 141/1203 - LOSS: 0.6602 - ACC: 0.8125\n",
      "\tBATCH 161/1203 - LOSS: 1.0781 - ACC: 0.5000\n",
      "\tBATCH 181/1203 - LOSS: 0.8984 - ACC: 0.6250\n",
      "\tBATCH 201/1203 - LOSS: 1.2891 - ACC: 0.5625\n",
      "\tBATCH 221/1203 - LOSS: 0.7422 - ACC: 0.6875\n",
      "\tBATCH 241/1203 - LOSS: 1.2500 - ACC: 0.5625\n",
      "\tBATCH 261/1203 - LOSS: 0.7344 - ACC: 0.7500\n",
      "\tBATCH 281/1203 - LOSS: 1.1562 - ACC: 0.5000\n",
      "\tBATCH 301/1203 - LOSS: 1.1172 - ACC: 0.5625\n",
      "\tBATCH 321/1203 - LOSS: 0.6914 - ACC: 0.8750\n",
      "\tBATCH 341/1203 - LOSS: 0.6016 - ACC: 0.8125\n",
      "\tBATCH 361/1203 - LOSS: 1.2656 - ACC: 0.5000\n",
      "\tBATCH 381/1203 - LOSS: 0.9180 - ACC: 0.5625\n",
      "\tBATCH 401/1203 - LOSS: 0.9492 - ACC: 0.6250\n",
      "\tBATCH 421/1203 - LOSS: 1.0781 - ACC: 0.6250\n",
      "\tBATCH 441/1203 - LOSS: 1.0938 - ACC: 0.5000\n",
      "\tBATCH 461/1203 - LOSS: 1.0469 - ACC: 0.5625\n",
      "\tBATCH 481/1203 - LOSS: 0.9297 - ACC: 0.6250\n",
      "\tBATCH 501/1203 - LOSS: 0.7969 - ACC: 0.5625\n",
      "\tBATCH 521/1203 - LOSS: 0.9062 - ACC: 0.6250\n",
      "\tBATCH 541/1203 - LOSS: 1.0391 - ACC: 0.6250\n",
      "\tBATCH 561/1203 - LOSS: 1.1016 - ACC: 0.5625\n",
      "\tBATCH 581/1203 - LOSS: 1.2656 - ACC: 0.5000\n",
      "\tBATCH 601/1203 - LOSS: 0.8555 - ACC: 0.6875\n",
      "\tBATCH 621/1203 - LOSS: 0.8047 - ACC: 0.7500\n",
      "\tBATCH 641/1203 - LOSS: 1.0469 - ACC: 0.6250\n",
      "\tBATCH 661/1203 - LOSS: 0.9141 - ACC: 0.7500\n",
      "\tBATCH 681/1203 - LOSS: 0.7617 - ACC: 0.8125\n",
      "\tBATCH 701/1203 - LOSS: 1.0859 - ACC: 0.6250\n",
      "\tBATCH 721/1203 - LOSS: 0.6328 - ACC: 0.8750\n",
      "\tBATCH 741/1203 - LOSS: 1.4219 - ACC: 0.4375\n",
      "\tBATCH 761/1203 - LOSS: 0.8242 - ACC: 0.6875\n",
      "\tBATCH 781/1203 - LOSS: 0.5156 - ACC: 0.8750\n",
      "\tBATCH 801/1203 - LOSS: 1.0781 - ACC: 0.6250\n",
      "\tBATCH 821/1203 - LOSS: 0.8477 - ACC: 0.7500\n",
      "\tBATCH 841/1203 - LOSS: 0.7695 - ACC: 0.7500\n",
      "\tBATCH 861/1203 - LOSS: 1.3672 - ACC: 0.3750\n",
      "\tBATCH 881/1203 - LOSS: 1.2578 - ACC: 0.5625\n",
      "\tBATCH 901/1203 - LOSS: 0.8008 - ACC: 0.8125\n",
      "\tBATCH 921/1203 - LOSS: 0.8281 - ACC: 0.7500\n",
      "\tBATCH 941/1203 - LOSS: 1.2891 - ACC: 0.5625\n",
      "\tBATCH 961/1203 - LOSS: 0.9688 - ACC: 0.5625\n",
      "\tBATCH 981/1203 - LOSS: 1.2031 - ACC: 0.5625\n",
      "\tBATCH 1001/1203 - LOSS: 0.9766 - ACC: 0.6250\n",
      "\tBATCH 1021/1203 - LOSS: 1.3672 - ACC: 0.5000\n",
      "\tBATCH 1041/1203 - LOSS: 1.3359 - ACC: 0.3750\n",
      "\tBATCH 1061/1203 - LOSS: 1.1562 - ACC: 0.5000\n",
      "\tBATCH 1081/1203 - LOSS: 0.9102 - ACC: 0.7500\n",
      "\tBATCH 1101/1203 - LOSS: 1.0234 - ACC: 0.6250\n",
      "\tBATCH 1121/1203 - LOSS: 1.1094 - ACC: 0.6250\n",
      "\tBATCH 1141/1203 - LOSS: 1.1797 - ACC: 0.5000\n",
      "\tBATCH 1161/1203 - LOSS: 0.5938 - ACC: 0.8125\n",
      "\tBATCH 1181/1203 - LOSS: 1.0781 - ACC: 0.5000\n",
      "\tBATCH 1201/1203 - LOSS: 1.3047 - ACC: 0.5000\n",
      "\n",
      "\t[TRAIN] EPOCH 4 - LOSS: 0.9855, ACCURACY: 0.6446\n",
      "\n",
      "EPOCH 4 - VALIDATING...\n",
      "\t[VALID] LOSS: 0.9836, ACCURACY: 0.6480\n",
      "\n",
      "Validation loss improved (1.0226 --> 0.9836). Saving model...\n",
      "==================================================\n",
      "EPOCH 5 - TRAINING...\n",
      "\tBATCH 1/1203 - LOSS: 0.9609 - ACC: 0.6875\n",
      "\tBATCH 21/1203 - LOSS: 0.8711 - ACC: 0.7500\n",
      "\tBATCH 41/1203 - LOSS: 1.1641 - ACC: 0.6250\n",
      "\tBATCH 61/1203 - LOSS: 0.9883 - ACC: 0.6250\n",
      "\tBATCH 81/1203 - LOSS: 1.0625 - ACC: 0.5000\n",
      "\tBATCH 101/1203 - LOSS: 1.1016 - ACC: 0.5000\n",
      "\tBATCH 121/1203 - LOSS: 1.2969 - ACC: 0.4375\n",
      "\tBATCH 141/1203 - LOSS: 0.6562 - ACC: 0.7500\n",
      "\tBATCH 161/1203 - LOSS: 1.0938 - ACC: 0.5625\n",
      "\tBATCH 181/1203 - LOSS: 0.8398 - ACC: 0.6250\n",
      "\tBATCH 201/1203 - LOSS: 1.3281 - ACC: 0.5625\n",
      "\tBATCH 221/1203 - LOSS: 0.7422 - ACC: 0.6875\n",
      "\tBATCH 241/1203 - LOSS: 1.1719 - ACC: 0.5625\n",
      "\tBATCH 261/1203 - LOSS: 0.7578 - ACC: 0.7500\n",
      "\tBATCH 281/1203 - LOSS: 1.0781 - ACC: 0.5625\n",
      "\tBATCH 301/1203 - LOSS: 1.0391 - ACC: 0.6875\n",
      "\tBATCH 321/1203 - LOSS: 0.6055 - ACC: 0.8125\n",
      "\tBATCH 341/1203 - LOSS: 0.5820 - ACC: 0.8750\n",
      "\tBATCH 361/1203 - LOSS: 1.2656 - ACC: 0.5000\n",
      "\tBATCH 381/1203 - LOSS: 0.9102 - ACC: 0.5625\n",
      "\tBATCH 401/1203 - LOSS: 0.9883 - ACC: 0.6250\n",
      "\tBATCH 421/1203 - LOSS: 1.0703 - ACC: 0.6250\n",
      "\tBATCH 441/1203 - LOSS: 1.1328 - ACC: 0.5625\n",
      "\tBATCH 461/1203 - LOSS: 1.0781 - ACC: 0.5625\n",
      "\tBATCH 481/1203 - LOSS: 1.0312 - ACC: 0.5625\n",
      "\tBATCH 501/1203 - LOSS: 0.9570 - ACC: 0.5625\n",
      "\tBATCH 521/1203 - LOSS: 0.9062 - ACC: 0.6250\n",
      "\tBATCH 541/1203 - LOSS: 1.0703 - ACC: 0.5000\n",
      "\tBATCH 561/1203 - LOSS: 1.0000 - ACC: 0.5625\n",
      "\tBATCH 581/1203 - LOSS: 1.2500 - ACC: 0.5000\n",
      "\tBATCH 601/1203 - LOSS: 0.9648 - ACC: 0.6250\n",
      "\tBATCH 621/1203 - LOSS: 0.8750 - ACC: 0.6875\n",
      "\tBATCH 641/1203 - LOSS: 1.1172 - ACC: 0.6875\n",
      "\tBATCH 661/1203 - LOSS: 0.9102 - ACC: 0.7500\n",
      "\tBATCH 681/1203 - LOSS: 0.7070 - ACC: 0.8125\n",
      "\tBATCH 701/1203 - LOSS: 0.9844 - ACC: 0.5625\n",
      "\tBATCH 721/1203 - LOSS: 0.5625 - ACC: 0.8750\n",
      "\tBATCH 741/1203 - LOSS: 1.4297 - ACC: 0.3750\n",
      "\tBATCH 761/1203 - LOSS: 0.9375 - ACC: 0.6250\n",
      "\tBATCH 781/1203 - LOSS: 0.5469 - ACC: 0.8750\n",
      "\tBATCH 801/1203 - LOSS: 1.1172 - ACC: 0.5625\n",
      "\tBATCH 821/1203 - LOSS: 0.8438 - ACC: 0.7500\n",
      "\tBATCH 841/1203 - LOSS: 0.9062 - ACC: 0.6875\n",
      "\tBATCH 861/1203 - LOSS: 1.3984 - ACC: 0.4375\n",
      "\tBATCH 881/1203 - LOSS: 1.1172 - ACC: 0.6250\n",
      "\tBATCH 901/1203 - LOSS: 0.7578 - ACC: 0.8125\n",
      "\tBATCH 921/1203 - LOSS: 0.8008 - ACC: 0.7500\n",
      "\tBATCH 941/1203 - LOSS: 1.2266 - ACC: 0.6250\n",
      "\tBATCH 961/1203 - LOSS: 1.0703 - ACC: 0.4375\n",
      "\tBATCH 981/1203 - LOSS: 1.2500 - ACC: 0.5000\n",
      "\tBATCH 1001/1203 - LOSS: 1.0703 - ACC: 0.5625\n",
      "\tBATCH 1021/1203 - LOSS: 1.3438 - ACC: 0.5000\n",
      "\tBATCH 1041/1203 - LOSS: 1.1953 - ACC: 0.4375\n",
      "\tBATCH 1061/1203 - LOSS: 1.2188 - ACC: 0.4375\n",
      "\tBATCH 1081/1203 - LOSS: 0.8516 - ACC: 0.8125\n",
      "\tBATCH 1101/1203 - LOSS: 1.0469 - ACC: 0.6875\n",
      "\tBATCH 1121/1203 - LOSS: 1.0156 - ACC: 0.5625\n",
      "\tBATCH 1141/1203 - LOSS: 1.0547 - ACC: 0.5000\n",
      "\tBATCH 1161/1203 - LOSS: 0.5391 - ACC: 0.8125\n",
      "\tBATCH 1181/1203 - LOSS: 0.8867 - ACC: 0.6250\n",
      "\tBATCH 1201/1203 - LOSS: 1.2266 - ACC: 0.6250\n",
      "\n",
      "\t[TRAIN] EPOCH 5 - LOSS: 0.9636, ACCURACY: 0.6486\n",
      "\n",
      "EPOCH 5 - VALIDATING...\n",
      "\t[VALID] LOSS: 0.9692, ACCURACY: 0.6546\n",
      "\n",
      "Validation loss improved (0.9836 --> 0.9692). Saving model...\n",
      "==================================================\n",
      "EPOCH 6 - TRAINING...\n",
      "\tBATCH 1/1203 - LOSS: 0.8203 - ACC: 0.6875\n",
      "\tBATCH 21/1203 - LOSS: 0.8203 - ACC: 0.7500\n",
      "\tBATCH 41/1203 - LOSS: 1.2578 - ACC: 0.6250\n",
      "\tBATCH 61/1203 - LOSS: 0.8594 - ACC: 0.6875\n",
      "\tBATCH 81/1203 - LOSS: 1.0859 - ACC: 0.5625\n",
      "\tBATCH 101/1203 - LOSS: 1.3047 - ACC: 0.5625\n",
      "\tBATCH 121/1203 - LOSS: 1.3047 - ACC: 0.4375\n",
      "\tBATCH 141/1203 - LOSS: 0.6211 - ACC: 0.8125\n",
      "\tBATCH 161/1203 - LOSS: 1.0859 - ACC: 0.5000\n",
      "\tBATCH 181/1203 - LOSS: 0.7578 - ACC: 0.6250\n",
      "\tBATCH 201/1203 - LOSS: 1.2578 - ACC: 0.5625\n",
      "\tBATCH 221/1203 - LOSS: 0.7461 - ACC: 0.6875\n",
      "\tBATCH 241/1203 - LOSS: 1.0547 - ACC: 0.6875\n",
      "\tBATCH 261/1203 - LOSS: 0.7109 - ACC: 0.7500\n",
      "\tBATCH 281/1203 - LOSS: 1.1484 - ACC: 0.5000\n",
      "\tBATCH 301/1203 - LOSS: 1.0000 - ACC: 0.6250\n",
      "\tBATCH 321/1203 - LOSS: 0.6094 - ACC: 0.8750\n",
      "\tBATCH 341/1203 - LOSS: 0.5859 - ACC: 0.8125\n",
      "\tBATCH 361/1203 - LOSS: 1.2969 - ACC: 0.5000\n",
      "\tBATCH 381/1203 - LOSS: 0.8477 - ACC: 0.7500\n",
      "\tBATCH 401/1203 - LOSS: 1.0000 - ACC: 0.5625\n",
      "\tBATCH 421/1203 - LOSS: 1.1484 - ACC: 0.6250\n",
      "\tBATCH 441/1203 - LOSS: 1.1094 - ACC: 0.5625\n",
      "\tBATCH 461/1203 - LOSS: 0.9570 - ACC: 0.5625\n",
      "\tBATCH 481/1203 - LOSS: 0.8672 - ACC: 0.6250\n",
      "\tBATCH 501/1203 - LOSS: 0.8047 - ACC: 0.6250\n",
      "\tBATCH 521/1203 - LOSS: 0.9688 - ACC: 0.5625\n",
      "\tBATCH 541/1203 - LOSS: 1.0938 - ACC: 0.5000\n",
      "\tBATCH 561/1203 - LOSS: 1.1875 - ACC: 0.5625\n",
      "\tBATCH 581/1203 - LOSS: 1.2734 - ACC: 0.5625\n",
      "\tBATCH 601/1203 - LOSS: 0.7383 - ACC: 0.6875\n",
      "\tBATCH 621/1203 - LOSS: 0.7422 - ACC: 0.7500\n",
      "\tBATCH 641/1203 - LOSS: 1.1875 - ACC: 0.6875\n",
      "\tBATCH 661/1203 - LOSS: 0.7812 - ACC: 0.7500\n",
      "\tBATCH 681/1203 - LOSS: 0.7188 - ACC: 0.8125\n",
      "\tBATCH 701/1203 - LOSS: 1.0859 - ACC: 0.5625\n",
      "\tBATCH 721/1203 - LOSS: 0.5586 - ACC: 0.8750\n",
      "\tBATCH 741/1203 - LOSS: 1.2734 - ACC: 0.3750\n",
      "\tBATCH 761/1203 - LOSS: 0.8555 - ACC: 0.6875\n",
      "\tBATCH 781/1203 - LOSS: 0.5156 - ACC: 0.9375\n",
      "\tBATCH 801/1203 - LOSS: 1.1562 - ACC: 0.5625\n",
      "\tBATCH 821/1203 - LOSS: 0.8828 - ACC: 0.7500\n",
      "\tBATCH 841/1203 - LOSS: 1.0156 - ACC: 0.6875\n",
      "\tBATCH 861/1203 - LOSS: 1.3672 - ACC: 0.5625\n",
      "\tBATCH 881/1203 - LOSS: 1.0234 - ACC: 0.5625\n",
      "\tBATCH 901/1203 - LOSS: 0.7539 - ACC: 0.8125\n",
      "\tBATCH 921/1203 - LOSS: 0.7852 - ACC: 0.8125\n",
      "\tBATCH 941/1203 - LOSS: 1.2812 - ACC: 0.6250\n",
      "\tBATCH 961/1203 - LOSS: 1.1016 - ACC: 0.5000\n",
      "\tBATCH 981/1203 - LOSS: 1.2734 - ACC: 0.5000\n",
      "\tBATCH 1001/1203 - LOSS: 0.8906 - ACC: 0.6875\n",
      "\tBATCH 1021/1203 - LOSS: 1.4453 - ACC: 0.5625\n",
      "\tBATCH 1041/1203 - LOSS: 1.3594 - ACC: 0.3750\n",
      "\tBATCH 1061/1203 - LOSS: 1.0156 - ACC: 0.5625\n",
      "\tBATCH 1081/1203 - LOSS: 0.7812 - ACC: 0.8125\n",
      "\tBATCH 1101/1203 - LOSS: 1.0859 - ACC: 0.6250\n",
      "\tBATCH 1121/1203 - LOSS: 1.0938 - ACC: 0.6250\n",
      "\tBATCH 1141/1203 - LOSS: 1.0547 - ACC: 0.5625\n",
      "\tBATCH 1161/1203 - LOSS: 0.5000 - ACC: 0.7500\n",
      "\tBATCH 1181/1203 - LOSS: 1.0156 - ACC: 0.6250\n",
      "\tBATCH 1201/1203 - LOSS: 1.2969 - ACC: 0.5625\n",
      "\n",
      "\t[TRAIN] EPOCH 6 - LOSS: 0.9517, ACCURACY: 0.6519\n",
      "\n",
      "EPOCH 6 - VALIDATING...\n",
      "\t[VALID] LOSS: 0.9589, ACCURACY: 0.6584\n",
      "\n",
      "Validation loss improved (0.9692 --> 0.9589). Saving model...\n",
      "==================================================\n",
      "EPOCH 7 - TRAINING...\n",
      "\tBATCH 1/1203 - LOSS: 0.9336 - ACC: 0.6875\n",
      "\tBATCH 21/1203 - LOSS: 0.7969 - ACC: 0.7500\n",
      "\tBATCH 41/1203 - LOSS: 1.1250 - ACC: 0.6250\n",
      "\tBATCH 61/1203 - LOSS: 0.7969 - ACC: 0.6250\n",
      "\tBATCH 81/1203 - LOSS: 1.0703 - ACC: 0.5625\n",
      "\tBATCH 101/1203 - LOSS: 1.0234 - ACC: 0.6250\n",
      "\tBATCH 121/1203 - LOSS: 1.1641 - ACC: 0.4375\n",
      "\tBATCH 141/1203 - LOSS: 0.6914 - ACC: 0.6875\n",
      "\tBATCH 161/1203 - LOSS: 0.9297 - ACC: 0.6250\n",
      "\tBATCH 181/1203 - LOSS: 0.8945 - ACC: 0.6250\n",
      "\tBATCH 201/1203 - LOSS: 1.2031 - ACC: 0.5625\n",
      "\tBATCH 221/1203 - LOSS: 0.7852 - ACC: 0.6875\n",
      "\tBATCH 241/1203 - LOSS: 1.2266 - ACC: 0.5625\n",
      "\tBATCH 261/1203 - LOSS: 0.6133 - ACC: 0.8125\n",
      "\tBATCH 281/1203 - LOSS: 1.1562 - ACC: 0.4375\n",
      "\tBATCH 301/1203 - LOSS: 0.9844 - ACC: 0.6250\n",
      "\tBATCH 321/1203 - LOSS: 0.6172 - ACC: 0.8750\n",
      "\tBATCH 341/1203 - LOSS: 0.5586 - ACC: 0.7500\n",
      "\tBATCH 361/1203 - LOSS: 1.2031 - ACC: 0.5000\n",
      "\tBATCH 381/1203 - LOSS: 0.8555 - ACC: 0.5625\n",
      "\tBATCH 401/1203 - LOSS: 0.8828 - ACC: 0.6875\n",
      "\tBATCH 421/1203 - LOSS: 0.9414 - ACC: 0.6250\n",
      "\tBATCH 441/1203 - LOSS: 1.0625 - ACC: 0.5000\n",
      "\tBATCH 461/1203 - LOSS: 1.1094 - ACC: 0.5625\n",
      "\tBATCH 481/1203 - LOSS: 0.8945 - ACC: 0.6250\n",
      "\tBATCH 501/1203 - LOSS: 0.9102 - ACC: 0.6250\n",
      "\tBATCH 521/1203 - LOSS: 0.7227 - ACC: 0.7500\n",
      "\tBATCH 541/1203 - LOSS: 1.1719 - ACC: 0.5000\n",
      "\tBATCH 561/1203 - LOSS: 1.1172 - ACC: 0.6250\n",
      "\tBATCH 581/1203 - LOSS: 1.5625 - ACC: 0.5000\n",
      "\tBATCH 601/1203 - LOSS: 0.8477 - ACC: 0.6875\n",
      "\tBATCH 621/1203 - LOSS: 0.8789 - ACC: 0.6875\n",
      "\tBATCH 641/1203 - LOSS: 1.2578 - ACC: 0.6250\n",
      "\tBATCH 661/1203 - LOSS: 0.8555 - ACC: 0.7500\n",
      "\tBATCH 681/1203 - LOSS: 0.6758 - ACC: 0.7500\n",
      "\tBATCH 701/1203 - LOSS: 0.9023 - ACC: 0.5625\n",
      "\tBATCH 721/1203 - LOSS: 0.5234 - ACC: 0.8125\n",
      "\tBATCH 741/1203 - LOSS: 1.5859 - ACC: 0.3750\n",
      "\tBATCH 761/1203 - LOSS: 0.8867 - ACC: 0.6250\n",
      "\tBATCH 781/1203 - LOSS: 0.4414 - ACC: 0.9375\n",
      "\tBATCH 801/1203 - LOSS: 1.0703 - ACC: 0.5625\n",
      "\tBATCH 821/1203 - LOSS: 0.8516 - ACC: 0.7500\n",
      "\tBATCH 841/1203 - LOSS: 0.8867 - ACC: 0.7500\n",
      "\tBATCH 861/1203 - LOSS: 1.6797 - ACC: 0.3750\n",
      "\tBATCH 881/1203 - LOSS: 1.0938 - ACC: 0.5625\n",
      "\tBATCH 901/1203 - LOSS: 0.8516 - ACC: 0.6875\n",
      "\tBATCH 921/1203 - LOSS: 0.8438 - ACC: 0.6875\n",
      "\tBATCH 941/1203 - LOSS: 1.2344 - ACC: 0.5000\n",
      "\tBATCH 961/1203 - LOSS: 1.1406 - ACC: 0.3750\n",
      "\tBATCH 981/1203 - LOSS: 1.2656 - ACC: 0.5625\n",
      "\tBATCH 1001/1203 - LOSS: 0.9414 - ACC: 0.7500\n",
      "\tBATCH 1021/1203 - LOSS: 1.3906 - ACC: 0.5625\n",
      "\tBATCH 1041/1203 - LOSS: 1.1562 - ACC: 0.5625\n",
      "\tBATCH 1061/1203 - LOSS: 1.2500 - ACC: 0.5000\n",
      "\tBATCH 1081/1203 - LOSS: 0.6875 - ACC: 0.8125\n",
      "\tBATCH 1101/1203 - LOSS: 1.0469 - ACC: 0.6250\n",
      "\tBATCH 1121/1203 - LOSS: 1.2188 - ACC: 0.4375\n",
      "\tBATCH 1141/1203 - LOSS: 1.0703 - ACC: 0.5625\n",
      "\tBATCH 1161/1203 - LOSS: 0.4863 - ACC: 0.8125\n",
      "\tBATCH 1181/1203 - LOSS: 0.9688 - ACC: 0.6875\n",
      "\tBATCH 1201/1203 - LOSS: 1.3125 - ACC: 0.6250\n",
      "\n",
      "\t[TRAIN] EPOCH 7 - LOSS: 0.9438, ACCURACY: 0.6551\n",
      "\n",
      "EPOCH 7 - VALIDATING...\n",
      "\t[VALID] LOSS: 0.9461, ACCURACY: 0.6570\n",
      "\n",
      "Validation loss improved (0.9589 --> 0.9461). Saving model...\n",
      "==================================================\n",
      "EPOCH 8 - TRAINING...\n",
      "\tBATCH 1/1203 - LOSS: 0.7852 - ACC: 0.6875\n",
      "\tBATCH 21/1203 - LOSS: 0.8203 - ACC: 0.7500\n",
      "\tBATCH 41/1203 - LOSS: 1.0781 - ACC: 0.6250\n",
      "\tBATCH 61/1203 - LOSS: 0.8438 - ACC: 0.6875\n",
      "\tBATCH 81/1203 - LOSS: 1.1719 - ACC: 0.6250\n",
      "\tBATCH 101/1203 - LOSS: 0.8984 - ACC: 0.8125\n",
      "\tBATCH 121/1203 - LOSS: 1.1484 - ACC: 0.4375\n",
      "\tBATCH 141/1203 - LOSS: 0.5781 - ACC: 0.8125\n",
      "\tBATCH 161/1203 - LOSS: 1.0859 - ACC: 0.5000\n",
      "\tBATCH 181/1203 - LOSS: 0.8125 - ACC: 0.6875\n",
      "\tBATCH 201/1203 - LOSS: 1.2422 - ACC: 0.5000\n",
      "\tBATCH 221/1203 - LOSS: 0.7617 - ACC: 0.6875\n",
      "\tBATCH 241/1203 - LOSS: 1.0547 - ACC: 0.6250\n",
      "\tBATCH 261/1203 - LOSS: 0.6211 - ACC: 0.8125\n",
      "\tBATCH 281/1203 - LOSS: 1.0938 - ACC: 0.5000\n",
      "\tBATCH 301/1203 - LOSS: 0.9844 - ACC: 0.6250\n",
      "\tBATCH 321/1203 - LOSS: 0.5703 - ACC: 0.8125\n",
      "\tBATCH 341/1203 - LOSS: 0.5469 - ACC: 0.8750\n",
      "\tBATCH 361/1203 - LOSS: 1.2891 - ACC: 0.5000\n",
      "\tBATCH 381/1203 - LOSS: 0.8672 - ACC: 0.6875\n",
      "\tBATCH 401/1203 - LOSS: 0.9844 - ACC: 0.7500\n",
      "\tBATCH 421/1203 - LOSS: 0.9688 - ACC: 0.6250\n",
      "\tBATCH 441/1203 - LOSS: 1.1562 - ACC: 0.5000\n",
      "\tBATCH 461/1203 - LOSS: 0.9688 - ACC: 0.5625\n",
      "\tBATCH 481/1203 - LOSS: 0.8125 - ACC: 0.5625\n",
      "\tBATCH 501/1203 - LOSS: 0.7109 - ACC: 0.6250\n",
      "\tBATCH 521/1203 - LOSS: 0.8789 - ACC: 0.6250\n",
      "\tBATCH 541/1203 - LOSS: 0.9961 - ACC: 0.5000\n",
      "\tBATCH 561/1203 - LOSS: 1.1562 - ACC: 0.6250\n",
      "\tBATCH 581/1203 - LOSS: 1.3516 - ACC: 0.5000\n",
      "\tBATCH 601/1203 - LOSS: 1.0234 - ACC: 0.6250\n",
      "\tBATCH 621/1203 - LOSS: 0.8906 - ACC: 0.6875\n",
      "\tBATCH 641/1203 - LOSS: 1.0312 - ACC: 0.6875\n",
      "\tBATCH 661/1203 - LOSS: 0.9258 - ACC: 0.7500\n",
      "\tBATCH 681/1203 - LOSS: 0.6641 - ACC: 0.7500\n",
      "\tBATCH 701/1203 - LOSS: 0.8320 - ACC: 0.6875\n",
      "\tBATCH 721/1203 - LOSS: 0.6172 - ACC: 0.8750\n",
      "\tBATCH 741/1203 - LOSS: 1.4453 - ACC: 0.4375\n",
      "\tBATCH 761/1203 - LOSS: 0.9453 - ACC: 0.6875\n",
      "\tBATCH 781/1203 - LOSS: 0.4551 - ACC: 0.9375\n",
      "\tBATCH 801/1203 - LOSS: 1.0703 - ACC: 0.5625\n",
      "\tBATCH 821/1203 - LOSS: 0.8750 - ACC: 0.7500\n",
      "\tBATCH 841/1203 - LOSS: 0.8477 - ACC: 0.6875\n",
      "\tBATCH 861/1203 - LOSS: 1.5312 - ACC: 0.3750\n",
      "\tBATCH 881/1203 - LOSS: 1.1641 - ACC: 0.6250\n",
      "\tBATCH 901/1203 - LOSS: 0.7383 - ACC: 0.8125\n",
      "\tBATCH 921/1203 - LOSS: 0.7969 - ACC: 0.7500\n",
      "\tBATCH 941/1203 - LOSS: 1.0312 - ACC: 0.6250\n",
      "\tBATCH 961/1203 - LOSS: 1.1797 - ACC: 0.5000\n",
      "\tBATCH 981/1203 - LOSS: 1.2188 - ACC: 0.5000\n",
      "\tBATCH 1001/1203 - LOSS: 0.9219 - ACC: 0.6250\n",
      "\tBATCH 1021/1203 - LOSS: 1.3359 - ACC: 0.5000\n",
      "\tBATCH 1041/1203 - LOSS: 1.2344 - ACC: 0.5000\n",
      "\tBATCH 1061/1203 - LOSS: 1.1484 - ACC: 0.5625\n",
      "\tBATCH 1081/1203 - LOSS: 0.8672 - ACC: 0.8125\n",
      "\tBATCH 1101/1203 - LOSS: 0.9609 - ACC: 0.7500\n",
      "\tBATCH 1121/1203 - LOSS: 0.9648 - ACC: 0.6250\n",
      "\tBATCH 1141/1203 - LOSS: 1.1016 - ACC: 0.6250\n",
      "\tBATCH 1161/1203 - LOSS: 0.4434 - ACC: 0.8750\n",
      "\tBATCH 1181/1203 - LOSS: 0.9180 - ACC: 0.6250\n",
      "\tBATCH 1201/1203 - LOSS: 1.2109 - ACC: 0.6250\n",
      "\n",
      "\t[TRAIN] EPOCH 8 - LOSS: 0.9348, ACCURACY: 0.6564\n",
      "\n",
      "EPOCH 8 - VALIDATING...\n",
      "\t[VALID] LOSS: 0.9450, ACCURACY: 0.6560\n",
      "\n",
      "Validation loss improved (0.9461 --> 0.9450). Saving model...\n",
      "==================================================\n",
      "EPOCH 9 - TRAINING...\n",
      "\tBATCH 1/1203 - LOSS: 0.9219 - ACC: 0.6875\n",
      "\tBATCH 21/1203 - LOSS: 0.6797 - ACC: 0.7500\n",
      "\tBATCH 41/1203 - LOSS: 1.2734 - ACC: 0.6250\n",
      "\tBATCH 61/1203 - LOSS: 0.8164 - ACC: 0.6875\n",
      "\tBATCH 81/1203 - LOSS: 0.9219 - ACC: 0.5625\n",
      "\tBATCH 101/1203 - LOSS: 0.9570 - ACC: 0.6875\n",
      "\tBATCH 121/1203 - LOSS: 1.2188 - ACC: 0.5000\n",
      "\tBATCH 141/1203 - LOSS: 0.5625 - ACC: 0.8125\n",
      "\tBATCH 161/1203 - LOSS: 0.8203 - ACC: 0.6250\n",
      "\tBATCH 181/1203 - LOSS: 0.6875 - ACC: 0.6875\n",
      "\tBATCH 201/1203 - LOSS: 1.1094 - ACC: 0.5625\n",
      "\tBATCH 221/1203 - LOSS: 0.7383 - ACC: 0.6875\n",
      "\tBATCH 241/1203 - LOSS: 1.1797 - ACC: 0.5625\n",
      "\tBATCH 261/1203 - LOSS: 0.7227 - ACC: 0.7500\n",
      "\tBATCH 281/1203 - LOSS: 1.0078 - ACC: 0.5625\n",
      "\tBATCH 301/1203 - LOSS: 1.0000 - ACC: 0.5625\n",
      "\tBATCH 321/1203 - LOSS: 0.5430 - ACC: 0.8125\n",
      "\tBATCH 341/1203 - LOSS: 0.5469 - ACC: 0.9375\n",
      "\tBATCH 361/1203 - LOSS: 1.1953 - ACC: 0.5000\n",
      "\tBATCH 381/1203 - LOSS: 0.8125 - ACC: 0.6875\n",
      "\tBATCH 401/1203 - LOSS: 0.9961 - ACC: 0.6250\n",
      "\tBATCH 421/1203 - LOSS: 1.0000 - ACC: 0.6250\n",
      "\tBATCH 441/1203 - LOSS: 1.1328 - ACC: 0.5000\n",
      "\tBATCH 461/1203 - LOSS: 0.9883 - ACC: 0.6250\n",
      "\tBATCH 481/1203 - LOSS: 0.7227 - ACC: 0.6250\n",
      "\tBATCH 501/1203 - LOSS: 0.8711 - ACC: 0.5625\n",
      "\tBATCH 521/1203 - LOSS: 0.7891 - ACC: 0.6875\n",
      "\tBATCH 541/1203 - LOSS: 1.2344 - ACC: 0.4375\n",
      "\tBATCH 561/1203 - LOSS: 0.9883 - ACC: 0.6250\n",
      "\tBATCH 581/1203 - LOSS: 1.4688 - ACC: 0.5625\n",
      "\tBATCH 601/1203 - LOSS: 0.9375 - ACC: 0.6875\n",
      "\tBATCH 621/1203 - LOSS: 0.6719 - ACC: 0.7500\n",
      "\tBATCH 641/1203 - LOSS: 0.9883 - ACC: 0.6250\n",
      "\tBATCH 661/1203 - LOSS: 0.8086 - ACC: 0.8125\n",
      "\tBATCH 681/1203 - LOSS: 0.6602 - ACC: 0.8125\n",
      "\tBATCH 701/1203 - LOSS: 0.7891 - ACC: 0.6875\n",
      "\tBATCH 721/1203 - LOSS: 0.5547 - ACC: 0.8750\n",
      "\tBATCH 741/1203 - LOSS: 1.5156 - ACC: 0.3125\n",
      "\tBATCH 761/1203 - LOSS: 1.0156 - ACC: 0.6250\n",
      "\tBATCH 781/1203 - LOSS: 0.7188 - ACC: 0.8750\n",
      "\tBATCH 801/1203 - LOSS: 1.0234 - ACC: 0.5625\n",
      "\tBATCH 821/1203 - LOSS: 1.0547 - ACC: 0.7500\n",
      "\tBATCH 841/1203 - LOSS: 0.8320 - ACC: 0.6875\n",
      "\tBATCH 861/1203 - LOSS: 1.2656 - ACC: 0.5000\n",
      "\tBATCH 881/1203 - LOSS: 1.1797 - ACC: 0.5625\n",
      "\tBATCH 901/1203 - LOSS: 0.6953 - ACC: 0.8125\n",
      "\tBATCH 921/1203 - LOSS: 0.7422 - ACC: 0.7500\n",
      "\tBATCH 941/1203 - LOSS: 1.1172 - ACC: 0.6875\n",
      "\tBATCH 961/1203 - LOSS: 1.0625 - ACC: 0.5625\n",
      "\tBATCH 981/1203 - LOSS: 1.2422 - ACC: 0.5625\n",
      "\tBATCH 1001/1203 - LOSS: 0.9883 - ACC: 0.5625\n",
      "\tBATCH 1021/1203 - LOSS: 1.2500 - ACC: 0.5000\n",
      "\tBATCH 1041/1203 - LOSS: 1.2422 - ACC: 0.5000\n",
      "\tBATCH 1061/1203 - LOSS: 1.1562 - ACC: 0.5625\n",
      "\tBATCH 1081/1203 - LOSS: 0.7969 - ACC: 0.7500\n",
      "\tBATCH 1101/1203 - LOSS: 1.0938 - ACC: 0.7500\n",
      "\tBATCH 1121/1203 - LOSS: 1.1094 - ACC: 0.5625\n",
      "\tBATCH 1141/1203 - LOSS: 0.9961 - ACC: 0.5625\n",
      "\tBATCH 1161/1203 - LOSS: 0.3730 - ACC: 0.8750\n",
      "\tBATCH 1181/1203 - LOSS: 0.8984 - ACC: 0.5625\n",
      "\tBATCH 1201/1203 - LOSS: 1.2344 - ACC: 0.6250\n",
      "\n",
      "\t[TRAIN] EPOCH 9 - LOSS: 0.9198, ACCURACY: 0.6602\n",
      "\n",
      "EPOCH 9 - VALIDATING...\n",
      "\t[VALID] LOSS: 0.9386, ACCURACY: 0.6579\n",
      "\n",
      "Validation loss improved (0.9450 --> 0.9386). Saving model...\n",
      "==================================================\n",
      "EPOCH 10 - TRAINING...\n",
      "\tBATCH 1/1203 - LOSS: 0.9922 - ACC: 0.5625\n",
      "\tBATCH 21/1203 - LOSS: 0.6562 - ACC: 0.7500\n",
      "\tBATCH 41/1203 - LOSS: 1.2344 - ACC: 0.6250\n",
      "\tBATCH 61/1203 - LOSS: 0.8242 - ACC: 0.6875\n",
      "\tBATCH 81/1203 - LOSS: 1.0312 - ACC: 0.4375\n",
      "\tBATCH 101/1203 - LOSS: 1.0469 - ACC: 0.6875\n",
      "\tBATCH 121/1203 - LOSS: 1.2031 - ACC: 0.4375\n",
      "\tBATCH 141/1203 - LOSS: 0.6055 - ACC: 0.8125\n",
      "\tBATCH 161/1203 - LOSS: 0.9258 - ACC: 0.5625\n",
      "\tBATCH 181/1203 - LOSS: 0.7930 - ACC: 0.6250\n",
      "\tBATCH 201/1203 - LOSS: 1.1484 - ACC: 0.5625\n",
      "\tBATCH 221/1203 - LOSS: 0.6523 - ACC: 0.7500\n",
      "\tBATCH 241/1203 - LOSS: 1.1250 - ACC: 0.6250\n",
      "\tBATCH 261/1203 - LOSS: 0.7344 - ACC: 0.7500\n",
      "\tBATCH 281/1203 - LOSS: 1.1484 - ACC: 0.4375\n",
      "\tBATCH 301/1203 - LOSS: 0.9062 - ACC: 0.5625\n",
      "\tBATCH 321/1203 - LOSS: 0.6016 - ACC: 0.8125\n",
      "\tBATCH 341/1203 - LOSS: 0.5156 - ACC: 0.8750\n",
      "\tBATCH 361/1203 - LOSS: 1.2734 - ACC: 0.5000\n",
      "\tBATCH 381/1203 - LOSS: 0.8555 - ACC: 0.6875\n",
      "\tBATCH 401/1203 - LOSS: 1.0234 - ACC: 0.6875\n",
      "\tBATCH 421/1203 - LOSS: 0.8359 - ACC: 0.6250\n",
      "\tBATCH 441/1203 - LOSS: 1.0859 - ACC: 0.5000\n",
      "\tBATCH 461/1203 - LOSS: 0.9844 - ACC: 0.6250\n",
      "\tBATCH 481/1203 - LOSS: 0.9336 - ACC: 0.5625\n",
      "\tBATCH 501/1203 - LOSS: 0.8398 - ACC: 0.5625\n",
      "\tBATCH 521/1203 - LOSS: 0.8047 - ACC: 0.6250\n",
      "\tBATCH 541/1203 - LOSS: 0.9570 - ACC: 0.5000\n",
      "\tBATCH 561/1203 - LOSS: 1.0234 - ACC: 0.6250\n",
      "\tBATCH 581/1203 - LOSS: 1.3281 - ACC: 0.5625\n",
      "\tBATCH 601/1203 - LOSS: 0.8398 - ACC: 0.5625\n",
      "\tBATCH 621/1203 - LOSS: 0.8164 - ACC: 0.6250\n",
      "\tBATCH 641/1203 - LOSS: 1.1797 - ACC: 0.6250\n",
      "\tBATCH 661/1203 - LOSS: 0.8711 - ACC: 0.8125\n",
      "\tBATCH 681/1203 - LOSS: 0.5586 - ACC: 0.8125\n",
      "\tBATCH 701/1203 - LOSS: 0.9648 - ACC: 0.5625\n",
      "\tBATCH 721/1203 - LOSS: 0.4941 - ACC: 0.8750\n",
      "\tBATCH 741/1203 - LOSS: 1.4688 - ACC: 0.3125\n",
      "\tBATCH 761/1203 - LOSS: 0.8867 - ACC: 0.6250\n",
      "\tBATCH 781/1203 - LOSS: 0.4961 - ACC: 0.8750\n",
      "\tBATCH 801/1203 - LOSS: 1.0391 - ACC: 0.5625\n",
      "\tBATCH 821/1203 - LOSS: 0.7891 - ACC: 0.7500\n",
      "\tBATCH 841/1203 - LOSS: 0.8945 - ACC: 0.6250\n",
      "\tBATCH 861/1203 - LOSS: 1.4609 - ACC: 0.3750\n",
      "\tBATCH 881/1203 - LOSS: 1.2891 - ACC: 0.5625\n",
      "\tBATCH 901/1203 - LOSS: 0.6680 - ACC: 0.8125\n",
      "\tBATCH 921/1203 - LOSS: 0.6055 - ACC: 0.8125\n",
      "\tBATCH 941/1203 - LOSS: 1.1953 - ACC: 0.5000\n",
      "\tBATCH 961/1203 - LOSS: 1.0703 - ACC: 0.6875\n",
      "\tBATCH 981/1203 - LOSS: 1.1172 - ACC: 0.5625\n",
      "\tBATCH 1001/1203 - LOSS: 1.0312 - ACC: 0.6250\n",
      "\tBATCH 1021/1203 - LOSS: 1.1484 - ACC: 0.5625\n",
      "\tBATCH 1041/1203 - LOSS: 1.1328 - ACC: 0.5000\n",
      "\tBATCH 1061/1203 - LOSS: 1.1875 - ACC: 0.6250\n",
      "\tBATCH 1081/1203 - LOSS: 0.7305 - ACC: 0.8125\n",
      "\tBATCH 1101/1203 - LOSS: 1.0234 - ACC: 0.6250\n",
      "\tBATCH 1121/1203 - LOSS: 1.1328 - ACC: 0.5625\n",
      "\tBATCH 1141/1203 - LOSS: 0.9844 - ACC: 0.6250\n",
      "\tBATCH 1161/1203 - LOSS: 0.4355 - ACC: 0.8750\n",
      "\tBATCH 1181/1203 - LOSS: 0.9883 - ACC: 0.5625\n",
      "\tBATCH 1201/1203 - LOSS: 1.0781 - ACC: 0.6250\n",
      "\n",
      "\t[TRAIN] EPOCH 10 - LOSS: 0.9182, ACCURACY: 0.6622\n",
      "\n",
      "EPOCH 10 - VALIDATING...\n",
      "\t[VALID] LOSS: 0.9301, ACCURACY: 0.6602\n",
      "\n",
      "Validation loss improved (0.9386 --> 0.9301). Saving model...\n",
      "Training completed in: 2:35:24.076107\n",
      "Final model saved as: model_final_20250722_1333.pth\n"
     ]
    }
   ],
   "source": [
    "def _mp_fn(rank, flags):\n",
    "    torch.set_default_tensor_type(\"torch.FloatTensor\")\n",
    "    _run()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Attempting TPU training...\")\n",
    "    \n",
    "    # TPU multiprocessing\n",
    "    try:\n",
    "        import torch_xla.distributed.xla_multiprocessing as xmp\n",
    "        FLAGS = {}\n",
    "        print(\"Starting TPU multiprocessing training...\")\n",
    "        xmp.spawn(_mp_fn, args=(FLAGS,), start_method=\"fork\")\n",
    "    except Exception as e:\n",
    "        print(f\"TPU multiprocessing failed: {e}\")\n",
    "        \n",
    "        # Opcion 2: single TPU core\n",
    "        try:\n",
    "            print(\"Trying single TPU core...\")\n",
    "            _run()\n",
    "        except Exception as e2:\n",
    "            print(f\"Single TPU failed: {e2}\")\n",
    "            \n",
    "            # Opcion 3: CPU/GPU\n",
    "            print(\"Falling back to CPU/GPU training...\")\n",
    "            _run()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "tpu1vmV38",
   "dataSources": [
    {
     "databundleVersionId": 1718836,
     "sourceId": 13836,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31091,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
